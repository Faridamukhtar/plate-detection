{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from commonfunctions import *\n",
    "import cv2 as cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import List\n",
    "\n",
    "\n",
    "def get_largest_contour_list(contour_lists: List[List]) -> List[List]:\n",
    "    # Handle empty input\n",
    "    if not contour_lists:\n",
    "        return []\n",
    "    if not contour_lists:\n",
    "        raise ValueError(\"Input must contain at least one list of contours.\")\n",
    "    \n",
    "    # Find the list with the maximum number of contours\n",
    "    largest_contour_list = max(contour_lists, key=len)\n",
    "    \n",
    "    # Return as a list containing only the largest contour list\n",
    "    return [largest_contour_list]\n",
    "\n",
    "# Function for bright RGB image enhancement\n",
    "def enhance_bright_image_rgb(image):\n",
    "    enhanced_image = image.copy()\n",
    "    for channel in range(3):  # Process R, G, B channels\n",
    "        for y in range(image.shape[0]):\n",
    "            for x in range(image.shape[1]):\n",
    "                pixel_value = image[y, x, channel]\n",
    "                if 200 <= pixel_value <= 230:\n",
    "                    # Scale pixel value between 240 and 255\n",
    "                    enhanced_image[y, x, channel] = 240 + (pixel_value - 200) * (255 - 240) // (230 - 200)\n",
    "                elif pixel_value > 230:\n",
    "                    # Keep values above 230 as they are\n",
    "                    enhanced_image[y, x, channel] = 255\n",
    "                else:\n",
    "                    # Keep values below 200 unchanged\n",
    "                    enhanced_image[y, x, channel] = pixel_value\n",
    "    return enhanced_image\n",
    "\n",
    "# Function for dark RGB image enhancement\n",
    "def enhance_dark_image_rgb(image):\n",
    "    enhanced_image = image.copy()\n",
    "    for channel in range(3):  # Process R, G, B channels\n",
    "        for y in range(image.shape[0]):\n",
    "            for x in range(image.shape[1]):\n",
    "                pixel_value = image[y, x, channel]\n",
    "                if 130 <= pixel_value <= 200:\n",
    "                    # Scale pixel value between 230 and 255\n",
    "                    enhanced_image[y, x, channel] = 230 + (pixel_value - 130) * (255 - 230) // (200 - 130)\n",
    "                elif pixel_value > 200:\n",
    "                    # Keep values above 200 as they are\n",
    "                    enhanced_image[y, x, channel] = 255\n",
    "                else:\n",
    "                    # Keep values below 130 unchanged\n",
    "                    enhanced_image[y, x, channel] = pixel_value\n",
    "    return enhanced_image\n",
    "\n",
    "def enhance_very_dark_image_rgb(image):\n",
    "    enhanced_image = image.copy()\n",
    "    for channel in range(3):  # Process R, G, B channels\n",
    "        for y in range(image.shape[0]):\n",
    "            for x in range(image.shape[1]):\n",
    "                pixel_value = image[y, x, channel]\n",
    "                if 90 <= pixel_value <= 150:\n",
    "                    # Scale pixel value between 230 and 250\n",
    "                    enhanced_image[y, x, channel] = 230 + (pixel_value - 90) * (250 - 230) // (150 - 90)\n",
    "                elif pixel_value > 150:\n",
    "                    # Keep values above 150 as they are\n",
    "                    enhanced_image[y, x, channel] = pixel_value\n",
    "                else:\n",
    "                    # Keep values below 90 unchanged\n",
    "                    enhanced_image[y, x, channel] = pixel_value\n",
    "    return enhanced_image\n",
    "\n",
    "def enhance_bright_image(image):\n",
    "    enhanced_image = image.copy()\n",
    "    for y in range(image.shape[0]):\n",
    "        for x in range(image.shape[1]):\n",
    "            pixel_value = image[y, x]\n",
    "            if 200 <= pixel_value <= 230:\n",
    "                # Scale pixel value between 240 and 255\n",
    "                enhanced_image[y, x] = 240 + (pixel_value - 200) * (255 - 240) // (230 - 200)\n",
    "            elif pixel_value > 230:\n",
    "                # Keep values above 230 as they are\n",
    "                enhanced_image[y, x] = 255\n",
    "            else:\n",
    "                # Keep values below 200 unchanged\n",
    "                enhanced_image[y, x] = pixel_value\n",
    "    return enhanced_image\n",
    "\n",
    "def enhance_dark_image(image):\n",
    "    enhanced_image = image.copy()\n",
    "    for y in range(image.shape[0]):\n",
    "        for x in range(image.shape[1]):\n",
    "            pixel_value = image[y, x]\n",
    "            if 130 <= pixel_value <= 200:\n",
    "                # Scale pixel value between 230 and 255\n",
    "                enhanced_image[y, x] = 230 + (pixel_value - 130) * (255 - 230) // (200 - 130)\n",
    "            elif pixel_value > 200:\n",
    "                # Keep values above 200 as they are\n",
    "                enhanced_image[y, x] = 255\n",
    "            else:\n",
    "                # Keep values below 130 unchanged\n",
    "                enhanced_image[y, x] = pixel_value\n",
    "    return enhanced_image\n",
    "\n",
    "def calculate_percentage_of_ones(binary_img):\n",
    "    \n",
    "\n",
    "    # Calculate the number of 1's\n",
    "    num_ones = np.count_nonzero(binary_img)\n",
    "\n",
    "    # Calculate the total number of pixels\n",
    "    total_pixels = binary_img.size\n",
    "\n",
    "    # Calculate the percentage of 1's\n",
    "    percentage_of_ones = (num_ones / total_pixels) * 100\n",
    "\n",
    "    return percentage_of_ones\n",
    "\n",
    "\n",
    "\n",
    "# Function for enhancing both dark and bright RGB image enhancement\n",
    "def enhance_image_rgb(image):\n",
    "    enhanced_image = image.copy()\n",
    "\n",
    "    for channel in range(3):  # Process R, G, B channels\n",
    "        for y in range(image.shape[0]):\n",
    "            for x in range(image.shape[1]):\n",
    "                pixel_value = image[y, x, channel]\n",
    "                \n",
    "                # Enhance dark to very dark pixel values (90 to 200)\n",
    "                if 90 <= pixel_value <= 200:\n",
    "                    if pixel_value <= 130:\n",
    "                        # Scale very dark pixels between 90 and 150 to 230 to 250\n",
    "                        enhanced_image[y, x, channel] = 230 + (pixel_value - 90) * (250 - 230) // (150 - 90)\n",
    "                    elif pixel_value <= 200:\n",
    "                        # Scale dark pixels between 130 and 200 to 230 to 255\n",
    "                        enhanced_image[y, x, channel] = 230 + (pixel_value - 130) * (255 - 230) // (200 - 130)\n",
    "                # Enhance bright pixels (200 to 230 range)\n",
    "                elif 200 <= pixel_value <= 230:\n",
    "                    # Scale bright pixels between 200 and 230 to 240 to 255\n",
    "                    enhanced_image[y, x, channel] = 240 + (pixel_value - 200) * (255 - 240) // (230 - 200)\n",
    "                elif pixel_value > 230:\n",
    "                    # Keep values above 230 as they are (already maximized)\n",
    "                    enhanced_image[y, x, channel] = 255\n",
    "                else:\n",
    "                    # Keep values below 90 unchanged (dark/black areas)\n",
    "                    enhanced_image[y, x, channel] = pixel_value\n",
    "\n",
    "    return enhanced_image\n",
    "\n",
    "def enhance_image(image):\n",
    "    enhanced_image = image.copy()\n",
    "\n",
    "    for y in range(image.shape[0]):\n",
    "        for x in range(image.shape[1]):\n",
    "            pixel_value = image[y, x]\n",
    "                \n",
    "            # Enhance dark to very dark pixel values (90 to 200)\n",
    "            if 90 <= pixel_value <= 200:\n",
    "                if pixel_value <= 130:\n",
    "                    # Scale very dark pixels between 90 and 150 to 230 to 250\n",
    "                    enhanced_image[y, x] = 230 + (pixel_value - 90) * (250 - 230) // (150 - 90)\n",
    "                elif pixel_value <= 200:\n",
    "                    # Scale dark pixels between 130 and 200 to 230 to 255\n",
    "                    enhanced_image[y, x] = 230 + (pixel_value - 130) * (255 - 230) // (200 - 130)\n",
    "            # Enhance bright pixels (200 to 230 range)\n",
    "            elif 200 <= pixel_value <= 230:\n",
    "                # Scale bright pixels between 200 and 230 to 240 to 255\n",
    "                enhanced_image[y, x] = 240 + (pixel_value - 200) * (255 - 240) // (230 - 200)\n",
    "            elif pixel_value > 230:\n",
    "                # Keep values above 230 as they are (already maximized)\n",
    "                enhanced_image[y, x] = 255\n",
    "            else:\n",
    "                # Keep values below 90 unchanged (dark/black areas)\n",
    "                enhanced_image[y, x] = pixel_value\n",
    "\n",
    "    return enhanced_image\n",
    "\n",
    "from scipy.signal import find_peaks, peak_widths\n",
    "def has_salt_and_pepper_noise(image):\n",
    "    # Convert to grayscale if needed\n",
    "    if len(image.shape) == 3:\n",
    "        image = cv.cvtColor(image, cv.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Flatten the image for histogram analysis\n",
    "    flattened_image = image.flatten()\n",
    "    hist, bins = np.histogram(flattened_image, bins=256, range=(0, 255))\n",
    "\n",
    "    # Detect peaks\n",
    "    peaks, properties = find_peaks(hist, prominence=0.1 * np.max(hist))  # Prominent peaks\n",
    "    widths = peak_widths(hist, peaks, rel_height=0.5)[0]  # Measure widths at half prominence\n",
    "\n",
    "    # Filter peaks that are narrow (spikes)\n",
    "    spike_indices = [i for i, width in enumerate(widths) if width <= 2]  # Adjust width threshold\n",
    "    spikes = peaks[spike_indices]\n",
    "\n",
    "    # Count spikes\n",
    "    num_spikes = len(spikes)\n",
    "    print(f\"Detected Impulses (Spikes): {num_spikes}\")\n",
    "\n",
    "    # Decision based on spike count\n",
    "    if num_spikes >= 5:  # Adjust threshold as needed\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "# checks whether the red channel is dominating \n",
    "def detect_dominant_red_channel(image, red_threshold=1.3):\n",
    "    # Ensure the image has 3 channels (RGB)\n",
    "    if image.shape[-1] != 3:\n",
    "        raise ValueError(\"Image must be an RGB image\")\n",
    "\n",
    "    # Split the image into R, G, B channels\n",
    "    R = image[:, :, 0]\n",
    "    G = image[:, :, 1]\n",
    "    B = image[:, :, 2]\n",
    "\n",
    "    # Calculate the mean intensity of each channel\n",
    "    mean_R = np.mean(R)\n",
    "    mean_G = np.mean(G)\n",
    "    mean_B = np.mean(B)\n",
    "\n",
    "    print(f\"Mean R: {mean_R:.2f}, Mean G: {mean_G:.2f}, Mean B: {mean_B:.2f}\")\n",
    "\n",
    "    # Check if the red channel is dominant compared to green and blue\n",
    "    if mean_R > red_threshold * mean_G and mean_R > red_threshold * mean_B:\n",
    "        print(\"The red channel is dominant.\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"The red channel is not dominant.\")\n",
    "        return False\n",
    "    \n",
    "# function used to equalize the red channel\n",
    "def equalizeredchannel(img):\n",
    "    img = img.astype(np.float32)\n",
    "    \n",
    "    red_channel = img[:, :, 0]\n",
    "    green_channel = img[:, :, 1]\n",
    "    blue_channel = img[:, :, 2]\n",
    "\n",
    "    original_red = np.clip(red_channel / 1.5, 0, 255).astype(np.uint8)\n",
    "    \n",
    "    equalized_img = np.stack((original_red, green_channel, blue_channel), axis=2).astype(np.uint8)\n",
    "    return equalized_img\n",
    "    \n",
    "# checks if the image contains additive noise or not\n",
    "def detect_gaussian_noise(image, threshold_std=10):\n",
    "    #  Convert to grayscale if needed\n",
    "    if len(image.shape) == 3:\n",
    "        gray = cv.cvtColor(image, cv.COLOR_BGR2GRAY)\n",
    "    else:\n",
    "        gray = image\n",
    "\n",
    "    # Smooth the image using Gaussian Blur\n",
    "    blurred = cv.GaussianBlur(gray, (5, 5), 1.0)\n",
    "\n",
    "    # Subtract the blurred image from the original image to isolate noise\n",
    "    noise = gray.astype(np.float32) - blurred.astype(np.float32)\n",
    "\n",
    "    # Calculate the standard deviation of the noise\n",
    "    noise_std = np.std(noise)\n",
    "    print(f\"Noise Standard Deviation: {noise_std:.2f}\")\n",
    "\n",
    "    # Threshold to classify noise\n",
    "    if noise_std > threshold_std:\n",
    "        print(\"Gaussian noise detected.\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"No significant Gaussian noise detected.\")\n",
    "        return False\n",
    "    \n",
    "    \n",
    "\n",
    "def Img_segmentation_and_enhancement(ImgPath):\n",
    "    img = cv.imread(ImgPath)\n",
    "\n",
    "    if img.shape[0] < 800 and img.shape[1] < 600:\n",
    "        img = cv.resize(img, (800, 600))\n",
    "    imgg = img\n",
    "    if has_salt_and_pepper_noise(img):\n",
    "        print(\"Salt and pepper noise detected!\")\n",
    "        img = median_filter(img, size = 3)\n",
    "    else:\n",
    "        print(\"No salt and pepper noise detected.\")\n",
    "    \n",
    "    # Assuming img is your input image\n",
    "    if img.dtype == np.float64:\n",
    "        # Convert the image to uint8\n",
    "        img = img.astype(np.uint8)\n",
    "    \n",
    "        # Or use .astype(np.uint8) directly if the values are already in [0, 255]\n",
    "    \n",
    "    img = cv.cvtColor(img, cv.COLOR_BGR2RGB)\n",
    "    image = np.copy(img)\n",
    "    if img.shape[2] == 4:\n",
    "        image = rgba2rgb(image)\n",
    "    if detect_dominant_red_channel(image):\n",
    "        print(\"Excessive red channel detected.\")\n",
    "        img = equalizeredchannel(img)\n",
    "    else:\n",
    "        print(\"No excessive red channel detected.\")\n",
    "    \n",
    "    if detect_gaussian_noise(img, threshold_std=15):\n",
    "        print(\"Image contains additive Gaussian noise.\")\n",
    "        img = gaussian(img, sigma = 1.3)\n",
    "    else:\n",
    "        print(\"Image does not contain significant Gaussian noise.\")\n",
    "    \n",
    "    \n",
    "    if len(img.shape) == 3:\n",
    "        height , width , channel = img.shape\n",
    "    elif len(img.shape) == 2:\n",
    "        height , width  = img.shape\n",
    "    \n",
    "    \n",
    "    gray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
    "    \n",
    "    \n",
    "    \n",
    "    img = enhance_image_rgb(img)\n",
    "    \n",
    "    \n",
    "    \n",
    "    TragetColor = np.array([240/255,240/255,240/255]) \n",
    "    \n",
    "    # 2- Read image\n",
    "    \n",
    "    img = img.astype(float) / 255\n",
    "    \n",
    "    # 3- Extract R, G, and B channels (as float)\n",
    "    RedChannel = img[:,:,0]\n",
    "    GreenChannel = img[:,:,1]\n",
    "    BlueChannel = img[:,:,2]\n",
    "    \n",
    "    # 4- Calculate differences FOR EACH CHANNEL (between the image and the required pixel value)\n",
    "    DiffRed = np.abs(RedChannel - TragetColor[0])\n",
    "    DiffGreen = np.abs(GreenChannel - TragetColor[1])\n",
    "    DiffBlue = np.abs(BlueChannel - TragetColor[2])\n",
    "    \n",
    "    # 5- Calculate overall distance from the given RGB color (use Euclidean distance)\n",
    "    Distance = np.sqrt(DiffRed**2 + DiffGreen**2 + DiffBlue**2)\n",
    "    \n",
    "    # 6- Create a mask by thresholding the differences\n",
    "    Threshold = 0.1\n",
    "    Mask = np.where(Distance < Threshold , True,False)\n",
    "    \n",
    "    # 7- Replace the pixels of the mask with the new color (R=230, G=90, B=40)\n",
    "    ReplacedColor = np.array([230/255, 90/255, 40/255])\n",
    "    \n",
    "    # Create a copy of the image to apply the changes\n",
    "    modified_img = np.copy(img)\n",
    "    \n",
    "    \n",
    "    struct_elem = np.ones((2, 3))\n",
    "    img_dilated1 = binary_dilation(Mask, struct_elem).astype(img.dtype)\n",
    "    \n",
    "    \n",
    "    struct_elem = np.ones((12, 4))\n",
    "    img_closed = binary_erosion(img_dilated1, struct_elem).astype(img.dtype)\n",
    "    \n",
    "    struct_elem = np.ones((70, 180))\n",
    "    img_dilated = binary_dilation(img_closed, struct_elem).astype(img.dtype)\n",
    "    \n",
    "    percentageOfOnes   = calculate_percentage_of_ones(img_dilated )\n",
    "    # Apply the new color where the mask is True\n",
    "    modified_img[Mask] = ReplacedColor\n",
    "    \n",
    "    return gray , img_dilated , percentageOfOnes , imgg\n",
    "\n",
    "\n",
    "\n",
    "def geting_contours_and_draw_it(gray , imgg):\n",
    "    # Convert to binary image using a simple threshold\n",
    "    ret, binary_img = cv.threshold(gray, 210, 255, cv.THRESH_BINARY)\n",
    "    thresholded_img_adaptive = cv.adaptiveThreshold(gray , maxValue=255.0 , adaptiveMethod=cv.ADAPTIVE_THRESH_MEAN_C\n",
    "                                            ,thresholdType=cv.THRESH_BINARY_INV, blockSize=71,C=71)\n",
    "    struct_elem = np.ones((50,50))\n",
    "    img_Dilatedd_adaptive = binary_dilation(binary_img, struct_elem).astype(gray.dtype)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Optionally, visualize contours on the labeled image\n",
    "    contours, _ = cv.findContours(binary_img, cv.RETR_CCOMP, cv.CHAIN_APPROX_SIMPLE)\n",
    "    contours_img = cv.drawContours(imgg.copy(), contours, -1, (0, 255, 0), 2)\n",
    "\n",
    "    return img_Dilatedd_adaptive\n",
    "\n",
    "\n",
    "def get_the_masked_img(gray , img_Dilatedd_adaptive , img_dilated , percentageOfOnes):\n",
    "    masked_img_dilated = np.zeros_like(gray)  # Create an empty array with the same shape as gray\n",
    "    masked_img_dilated[img_dilated >= 1] = gray[img_dilated >= 1]  # Retain the grayscale values where mask is 255\n",
    "\n",
    "    masked_img_dilated_adaptivaly = np.zeros_like(gray)  # Create an empty array with the same shape as gray\n",
    "    masked_img_dilated_adaptivaly[img_Dilatedd_adaptive >= 1] = gray[img_Dilatedd_adaptive >= 1]  # Retain the grayscale values where mask is 255\n",
    "\n",
    "\n",
    "    if percentageOfOnes >= 90 and percentageOfOnes <91:\n",
    "        output_img = masked_img_dilated_adaptivaly\n",
    "    elif percentageOfOnes >=99:\n",
    "        output_img = masked_img_dilated_adaptivaly\n",
    "    else:\n",
    "        output_img = masked_img_dilated\n",
    "\n",
    "    return output_img\n",
    "\n",
    "def get_thresholding_for_the_output_image(output_img):\n",
    "    blurred_img = cv.GaussianBlur(output_img , ksize=(5,5) , sigmaX=0)\n",
    "    thresholded_img = cv.adaptiveThreshold(blurred_img , maxValue=255.0 , adaptiveMethod=cv.ADAPTIVE_THRESH_MEAN_C\n",
    "                                            ,thresholdType=cv.THRESH_BINARY_INV, blockSize=19,C=9)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return thresholded_img\n",
    "\n",
    "\n",
    "def get_contours_and_draw_them_as_rectangles(thresholded_img,img):\n",
    "    if len(img.shape) == 3:\n",
    "        height , width , channel = img.shape\n",
    "    elif len(img.shape) == 2:\n",
    "        height , width  = img.shape\n",
    "\n",
    "\n",
    "    contours ,_ = cv.findContours(thresholded_img , mode = cv.RETR_LIST , method=cv.CHAIN_APPROX_SIMPLE)\n",
    "    temp_result = np.zeros((height,width , channel),dtype=np.uint8)\n",
    "    cv.drawContours(temp_result , contours=contours , contourIdx=-1 , color=(255,255,255))\n",
    "    \n",
    "\n",
    "    contours_dict=[]\n",
    "\n",
    "    for contour in contours:\n",
    "        x,y,w,h = cv.boundingRect(contour)\n",
    "        cv.rectangle(temp_result , pt1=(x,y),pt2=(x+w,y+h) , color=(255,255,255) , thickness=1)\n",
    "        #insert the rectangles into the dict\n",
    "        contours_dict.append({'contour' : contour , 'x':x,'y':y,'w':w,'h':h ,'cx':x+(w/2),'cy': y+(h/2)})\n",
    "        \n",
    "    \n",
    "    \n",
    "    return contours_dict\n",
    "\n",
    "\n",
    "\n",
    "def first_stage_of_filtering_contours(contours_dict ,img):\n",
    "\n",
    "    if len(img.shape) == 3:\n",
    "        height , width , channel = img.shape\n",
    "    elif len(img.shape) == 2:\n",
    "        height , width  = img.shape\n",
    "\n",
    "        \n",
    "    min_area = 149\n",
    "    max_area = 3000\n",
    "    min_width , min_height = 1,5\n",
    "    min_ratio , max_ratio = 0.25, 1.2\n",
    "    possible_contours = []\n",
    "\n",
    "    cnt = 0\n",
    "\n",
    "    for d in contours_dict:\n",
    "        area = d['w'] * d['h']\n",
    "        ratio = d['w'] / d['h']\n",
    "\n",
    "        if min_area<area <max_area  and d['w'] > min_width and d['h'] > min_height and min_ratio < ratio <max_ratio:\n",
    "            d['idx'] = cnt\n",
    "            cnt +=1\n",
    "            possible_contours.append(d)\n",
    "\n",
    "    temp_result = np.zeros((height,width,channel),dtype=np.uint8)\n",
    "    for d in possible_contours:\n",
    "        cv.rectangle(temp_result , pt1=(d['x'],d['y']) , pt2=(d['x']+d['w'] , d['y']+d['h']) , color=(255,255,255) , thickness=2)\n",
    "\n",
    "    \n",
    "    return possible_contours\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def find_chars(contours_list , max_diag_multiplyer ,max_angle_diff ,max_area_diff , max_width_diff ,  max_height_diff , min_n_matched):\n",
    "    matched_result_idx = []\n",
    "\n",
    "    for d1 in contours_list:\n",
    "        matched_contour_idx = []\n",
    "        for d2 in contours_list:\n",
    "            if d1['idx'] == d2['idx']:\n",
    "                continue\n",
    "\n",
    "            dx = abs(d1['cx'] - d2['cx'])\n",
    "            dy = abs(d1['cy'] - d2['cy'])\n",
    "\n",
    "            diagonal_length = np.sqrt(d1['w']**2 + d1['h']**2)\n",
    "            distance = np.linalg.norm(np.array([d1['cx'], d1['cy']]) - np.array([d2['cx'], d2['cy']]))\n",
    "\n",
    "            angle_diff = 90 if dx == 0 else np.degrees(np.arctan(dy / dx))\n",
    "            \n",
    "            area_diff = abs((d1['w'] * d1['h'] - d2['w'] * d2['h']) / (d1['w'] * d1['h']))\n",
    "            width_diff = abs((d1['w'] - d2['w']) / d1['w'])\n",
    "            height_diff = abs((d1['h'] - d2['h']) / d1['h'])\n",
    "\n",
    "            if distance < diagonal_length * max_diag_multiplyer and angle_diff < max_angle_diff \\\n",
    "                and area_diff < max_area_diff and width_diff < max_width_diff and height_diff < max_height_diff:\n",
    "                matched_contour_idx.append(d2['idx'])\n",
    "\n",
    "        if len(matched_contour_idx) < min_n_matched:\n",
    "            continue\n",
    "\n",
    "        matched_result_idx.append([d1['idx']] + matched_contour_idx)\n",
    "\n",
    "        unmatched_contour_idx = [d4['idx'] for d4 in contours_list if d4['idx'] not in matched_contour_idx]\n",
    "\n",
    "        if not unmatched_contour_idx:\n",
    "            break\n",
    "\n",
    "        unmatched_contour = np.array([d for d in contours_list if d['idx'] in unmatched_contour_idx])\n",
    "        recursive_contour_list = find_chars(unmatched_contour ,  max_diag_multiplyer ,max_angle_diff ,max_area_diff , max_width_diff ,  max_height_diff , min_n_matched)\n",
    "\n",
    "        for idx in recursive_contour_list:\n",
    "            matched_result_idx.append(idx)\n",
    "\n",
    "        break\n",
    "\n",
    "    return matched_result_idx\n",
    "\n",
    "\n",
    "\n",
    "def second_stage_of_contour_filtering(possible_contours , img):\n",
    "    if len(img.shape) == 3:\n",
    "        height , width , channel = img.shape\n",
    "    elif len(img.shape) == 2:\n",
    "        height , width  = img.shape\n",
    "\n",
    "        \n",
    "    result_idx = find_chars(possible_contours ,max_diag_multiplyer = 4.1 ,max_angle_diff = 25.0 ,max_area_diff = 1.47 ,max_width_diff = 1.5 ,max_height_diff = 0.18 ,min_n_matched = 4 )\n",
    "\n",
    "    matched_resultx = []\n",
    "\n",
    "    for idx_list in result_idx:\n",
    "        matched_resultx.append(np.take(possible_contours , idx_list))\n",
    "\n",
    "\n",
    "\n",
    "    temp_resultx = np.zeros((height,width,channel),dtype=np.uint8)\n",
    "    for r in matched_resultx:\n",
    "        for d in r:\n",
    "            \n",
    "            cv.rectangle(temp_resultx , pt1=(d['x'],d['y']) , pt2=(d['x']+d['w'] , d['y']+d['h']) , color=(255,255,255) , thickness=2)\n",
    "            \n",
    "   \n",
    "    temp_result = np.zeros((height,width,channel),dtype=np.uint8)\n",
    "\n",
    "    matched_result = get_largest_contour_list(matched_resultx)\n",
    "\n",
    "    for r in matched_result:\n",
    "        for d in r:\n",
    "            \n",
    "            cv.rectangle(temp_result , pt1=(d['x'],d['y']) , pt2=(d['x']+d['w'] , d['y']+d['h']) , color=(255,255,255) , thickness=2)\n",
    "\n",
    "    \n",
    "    return matched_result\n",
    "\n",
    "\n",
    "def Draw_contours_on_the_real_img(possible_contours , imgg):\n",
    "\n",
    "    if len(imgg.shape) == 3:\n",
    "        height , width , channel = imgg.shape\n",
    "    elif len(imgg.shape) == 2:\n",
    "        height , width  = imgg.shape\n",
    "\n",
    "    result_idx = find_chars(possible_contours ,max_diag_multiplyer = 4.1 ,max_angle_diff = 25.0 ,max_area_diff = 1.47 ,max_width_diff = 1.5 ,max_height_diff = 0.18 ,min_n_matched = 4 )\n",
    "\n",
    "\n",
    "    matched_resultx = []\n",
    "\n",
    "    for idx_list in result_idx:\n",
    "        matched_resultx.append(np.take(possible_contours , idx_list))\n",
    "\n",
    "\n",
    "    matched_result = get_largest_contour_list(matched_resultx)\n",
    "\n",
    "   \n",
    "\n",
    "    for r in matched_result:\n",
    "        for d in r:\n",
    "            \n",
    "            cv.rectangle(imgg , pt1=(d['x'],d['y']) , pt2=(d['x']+d['w'] , d['y']+d['h']) , color=(0,255,0) , thickness=2)\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "def third_stage_of_contour_filtering_and_plate_extraction(matched_result , thresholded_img , img):\n",
    "    if len(img.shape) == 3:\n",
    "        height , width , channel = img.shape\n",
    "    elif len(img.shape) == 2:\n",
    "        height , width  = img.shape\n",
    "\n",
    "        \n",
    "    plate_width_padding = 0.6\n",
    "    plate_height_padding = 1.5\n",
    "    min_plate_ratio =1\n",
    "    max_plate_ratio = 2\n",
    "\n",
    "\n",
    "    plate_imgs = []\n",
    "    plate_infos = []\n",
    "\n",
    "    for i , matched_chars in enumerate(matched_result):\n",
    "        sorted_chars = sorted(matched_chars , key=lambda x : x['cx'])\n",
    "\n",
    "        plate_cx = (sorted_chars[0]['cx'] + sorted_chars[-1]['cx']) /2\n",
    "        plate_cy = (sorted_chars[0]['cy'] + sorted_chars[-1]['cy']) /2\n",
    "\n",
    "        plate_width = (sorted_chars[-1]['x'] + sorted_chars[-1]['w'] - sorted_chars[0]['w'] ) * plate_width_padding\n",
    "\n",
    "        sum_height = 0\n",
    "        cropped_imgs = []\n",
    "        for d in sorted_chars:\n",
    "            sum_height += d['h']\n",
    "\n",
    "\n",
    "        plate_height = int(sum_height / len(sorted_chars) * plate_height_padding)\n",
    "\n",
    "        triangle_height = sorted_chars[-1]['cy'] - sorted_chars[0]['cy']\n",
    "        triangle_hypotenus = np.linalg.norm(\n",
    "            np.array([sorted_chars[0]['cx'] , sorted_chars[0]['cy']]) -\n",
    "            np.array([sorted_chars[-1]['cx'] , sorted_chars[-1]['cy']])\n",
    "        )\n",
    "\n",
    "        angle = np.degrees(np.arcsin(triangle_height / triangle_hypotenus))\n",
    "\n",
    "        rotation_matrix = cv.getRotationMatrix2D(center=(plate_cx , plate_cy) , angle=angle , scale=1.0)\n",
    "\n",
    "        img_rotated = cv.warpAffine(thresholded_img , M=rotation_matrix , dsize=(width, height))\n",
    "\n",
    "        img_cropped = cv.getRectSubPix(\n",
    "            img_rotated,\n",
    "            patchSize=(int(plate_width) , int(plate_height)),\n",
    "            center = (int(plate_cx) , int(plate_cy))\n",
    "        )\n",
    "\n",
    "        if img_cropped.shape[1] / img_cropped.shape[0] < min_plate_ratio or min_plate_ratio < img_cropped.shape[1] / img_cropped.shape[0] <  max_plate_ratio:\n",
    "            continue\n",
    "\n",
    "\n",
    "        plate_imgs.append(img_cropped)\n",
    "        \n",
    "        plate_infos.append({\n",
    "            'x':int(plate_cx - plate_width /2),\n",
    "            'y':int(plate_cy - plate_height /2),\n",
    "            'w':int(plate_width),\n",
    "            'h':int(plate_height)\n",
    "        })\n",
    "\n",
    "        \n",
    "        return img_cropped\n",
    "\n",
    "def First_stage_of_removing_noise_from_cropped_plate(img_cropped):\n",
    "    cropped = img_cropped\n",
    "\n",
    "    contours ,_ = cv.findContours(img_cropped , mode = cv.RETR_LIST , method=cv.CHAIN_APPROX_SIMPLE)\n",
    "    temp_result = np.zeros((img_cropped.shape[0],img_cropped.shape[1]),dtype=np.uint8)\n",
    "    cv.drawContours(temp_result , contours=contours , contourIdx=-1 , color=(255,255,255))\n",
    "    \n",
    "\n",
    "    temp_result = np.zeros((img_cropped.shape[0],img_cropped.shape[1]),dtype=np.uint8)\n",
    "    contours_dict=[]\n",
    "\n",
    "    for contour in contours:\n",
    "        x,y,w,h = cv.boundingRect(contour)\n",
    "        cv.rectangle(temp_result , pt1=(x,y),pt2=(x+w,y+h) , color=(255,255,255) , thickness=1)\n",
    "        #insert the rectangles into the dict\n",
    "        contours_dict.append({'contour' : contour , 'x':x,'y':y,'w':w,'h':h ,'cx':x+(w/2),'cy': y+(h/2)})\n",
    "    \n",
    "    return contours_dict ,cropped\n",
    "\n",
    "\n",
    "def Second_stage_of_removing_noise_from_cropped_plate(contours_dict , img_cropped):\n",
    "    min_area = 149\n",
    "    max_area = 2000\n",
    "    min_width , min_height = 1,5\n",
    "    min_ratio , max_ratio = 0.25 , 1.62\n",
    "\n",
    "    possible_contours = []\n",
    "\n",
    "    cnt = 0\n",
    "\n",
    "    for d in contours_dict:\n",
    "        area = d['w'] * d['h']\n",
    "        ratio = d['w'] / d['h']\n",
    "\n",
    "        if min_area<area <max_area  and d['w'] > min_width and d['h'] > min_height and min_ratio < ratio <max_ratio:\n",
    "            d['idx'] = cnt\n",
    "            cnt +=1\n",
    "            possible_contours.append(d)\n",
    "\n",
    "    temp_result = np.zeros((img_cropped.shape[0],img_cropped.shape[1]),dtype=np.uint8)\n",
    "    for d in possible_contours:\n",
    "        cv.rectangle(temp_result , pt1=(d['x'],d['y']) , pt2=(d['x']+d['w'] , d['y']+d['h']) , color=(255,255,255) , thickness=2)\n",
    "\n",
    "    \n",
    "    return possible_contours\n",
    "\n",
    "\n",
    "def remove_edge_rectangles(temp_result, matched_result):\n",
    "    # Get the height and width of the temp_result image\n",
    "    height, width = temp_result.shape[:2]\n",
    "    \n",
    "    # Create a new blank image to draw valid rectangles\n",
    "    updated_result = np.zeros_like(temp_result)\n",
    "    \n",
    "    # Filter rectangles that do not touch the edges\n",
    "    for r in matched_result:\n",
    "        for d in r:\n",
    "            x, y, w, h = d['x'], d['y'], d['w'], d['h']\n",
    "            \n",
    "            # Check if the rectangle touches any edge\n",
    "            if (x == 0 or x + w == width or  # Left or right edge\n",
    "                y == 0 or y + h == height): # Top or bottom edge\n",
    "                continue  # Skip rectangles that touch the edges\n",
    "            \n",
    "            # Draw valid rectangles on the updated_result image\n",
    "            cv.rectangle(updated_result, pt1=(x, y), pt2=(x + w, y + h), color=(255, 255, 255), thickness=cv.FILLED)\n",
    "    \n",
    "    return updated_result\n",
    "\n",
    "\n",
    "def Third_stage_of_removing_noise_from_cropped_plate(possible_contours , img_cropped):\n",
    "\n",
    "    result_idx = find_chars(possible_contours ,max_diag_multiplyer = 8 , max_angle_diff = 25.0 , max_area_diff = 10 ,max_width_diff = 5 ,max_height_diff = 0.2 ,min_n_matched = 2 )\n",
    "\n",
    "    matched_resultx = []\n",
    "\n",
    "    for idx_list in result_idx:\n",
    "        matched_resultx.append(np.take(possible_contours , idx_list))\n",
    "\n",
    "\n",
    "\n",
    "    temp_resultx = np.zeros((img_cropped.shape[0],img_cropped.shape[1]),dtype=np.uint8)\n",
    "    for r in matched_resultx:\n",
    "        for d in r:\n",
    "            \n",
    "            cv.rectangle(temp_resultx , pt1=(d['x'],d['y']) , pt2=(d['x']+d['w'] , d['y']+d['h']) , color=(255,255,255) , thickness=2)\n",
    "            \n",
    "    \n",
    "    temp_result = np.zeros((img_cropped.shape[0],img_cropped.shape[1]),dtype=np.uint8)\n",
    "\n",
    "    matched_result = get_largest_contour_list(matched_resultx)\n",
    "\n",
    "    for r in matched_result:\n",
    "        for d in r:\n",
    "            \n",
    "            cv.rectangle(temp_result , pt1=(d['x'],d['y']) , pt2=(d['x']+d['w'] , d['y']+d['h']) , color=(255,255,255) , thickness=cv.FILLED)\n",
    "\n",
    "\n",
    "    updated_temp = remove_edge_rectangles(temp_result ,matched_result )\n",
    "\n",
    "    \n",
    "    return temp_result ,updated_temp\n",
    "\n",
    "\n",
    "\n",
    "def apply_mask(image, mask):\n",
    "    # Ensure the mask is binary (0 or 1) and has the same shape as the image\n",
    "    mask = mask.astype(np.uint8)  # Convert mask to uint8 for safety\n",
    "    if mask.shape != image.shape:\n",
    "        raise ValueError(\"The image and mask must have the same shape\")\n",
    "\n",
    "    # Apply the mask to the image: element-wise multiplication\n",
    "    result = cv.bitwise_and(image, mask)\n",
    "    \n",
    "    return result\n",
    "\n",
    "def get_the_plate(cropped , temp_result , updated_temp):\n",
    "\n",
    "    image1 = apply_mask(cropped , temp_result)\n",
    "\n",
    "    image = apply_mask(cropped , updated_temp)\n",
    "    \n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_process_function(img_path):\n",
    "    gray , img_dilated , percentageOfOnes , imgg = Img_segmentation_and_enhancement(img_path)\n",
    "    img_Dilatedd_adaptive = geting_contours_and_draw_it(gray , imgg)\n",
    "    output_img =get_the_masked_img(gray, img_Dilatedd_adaptive , img_dilated ,percentageOfOnes )\n",
    "    thresholded_img = get_thresholding_for_the_output_image(output_img)\n",
    "    contours_dict = get_contours_and_draw_them_as_rectangles(thresholded_img , imgg)\n",
    "    possible_contours = first_stage_of_filtering_contours(contours_dict , imgg)\n",
    "    matched_result = second_stage_of_contour_filtering(possible_contours , imgg)\n",
    "    Draw_contours_on_the_real_img(possible_contours , imgg)\n",
    "    img_cropped = third_stage_of_contour_filtering_and_plate_extraction(matched_result,thresholded_img , imgg)\n",
    "    contours_dict , cropped =First_stage_of_removing_noise_from_cropped_plate(img_cropped)\n",
    "    possible_contours = Second_stage_of_removing_noise_from_cropped_plate(contours_dict ,img_cropped )\n",
    "    temp_result ,updated_temp =Third_stage_of_removing_noise_from_cropped_plate( possible_contours , img_cropped)\n",
    "    image = get_the_plate(cropped , temp_result , updated_temp)\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hog_features(img):\n",
    "    # Resize the input image to ensure consistent dimensions for feature extraction\n",
    "    target_img_size = (32, 32)\n",
    "    img = cv.resize(img, target_img_size)\n",
    "\n",
    "    # Define HOG parameters\n",
    "    win_size = (32, 32)\n",
    "    cell_size = (4, 4)\n",
    "    block_size_in_cells = (2, 2)\n",
    "    block_size = (block_size_in_cells[1] * cell_size[1], block_size_in_cells[0] * cell_size[0])\n",
    "    block_stride = (cell_size[1], cell_size[0])\n",
    "    nbins = 9\n",
    "\n",
    "    # The HOGDescriptor object is configured with the above parameters\n",
    "    hog = cv.HOGDescriptor(win_size, block_size, block_stride, cell_size, nbins)\n",
    "    \n",
    "    # The HOG features are calculated for the resized image\n",
    "    h = hog.compute(img)\n",
    "    \n",
    "    # HOG features are returned as a multi-dimensional array, flatten it into a 1D vector\n",
    "    h = h.flatten()\n",
    "\n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "\n",
    "    path_to_dataset = r'digits_dataset'\n",
    "\n",
    "    features = []\n",
    "    labels = []\n",
    "\n",
    "    # Iterate through subfolders (assume folder name is the label)\n",
    "    for label_folder in os.listdir(path_to_dataset):\n",
    "        label_path = os.path.join(path_to_dataset, label_folder)\n",
    "        \n",
    "        # Ensure it's a directory (skip files)\n",
    "        if not os.path.isdir(label_path):\n",
    "            continue\n",
    "        \n",
    "        # Extract label from the folder name\n",
    "        label = label_folder\n",
    "        \n",
    "        # Iterate through the images in the folder\n",
    "        for img_filename in os.listdir(label_path):\n",
    "            img_path = os.path.join(label_path, img_filename)\n",
    "            \n",
    "            # Read image\n",
    "            img = cv.imread(img_path)\n",
    "            \n",
    "            # Extract features (e.g., HOG features)\n",
    "            features.append(extract_hog_features(img))\n",
    "            \n",
    "            # Append the label\n",
    "            labels.append(label)\n",
    "            \n",
    "            # Show an update every 1,000 images\n",
    "            if len(features) % 1000 == 0:\n",
    "                print(f\"[INFO] Processed {len(features)} images so far.\")\n",
    "\n",
    "    return features, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset. This will take time ...\n",
      "[INFO] Processed 1000 images so far.\n",
      "[INFO] Processed 2000 images so far.\n",
      "[INFO] Processed 3000 images so far.\n",
      "[INFO] Processed 4000 images so far.\n",
      "[INFO] Processed 5000 images so far.\n",
      "[INFO] Processed 6000 images so far.\n",
      "[INFO] Processed 7000 images so far.\n",
      "[INFO] Processed 8000 images so far.\n",
      "[INFO] Processed 9000 images so far.\n",
      "[INFO] Processed 10000 images so far.\n",
      "[INFO] Processed 11000 images so far.\n",
      "[INFO] Processed 12000 images so far.\n",
      "[INFO] Processed 13000 images so far.\n",
      "[INFO] Processed 14000 images so far.\n",
      "[INFO] Processed 15000 images so far.\n",
      "[INFO] Processed 16000 images so far.\n",
      "[INFO] Processed 17000 images so far.\n",
      "[INFO] Processed 18000 images so far.\n",
      "[INFO] Processed 19000 images so far.\n",
      "[INFO] Processed 20000 images so far.\n",
      "[INFO] Processed 21000 images so far.\n",
      "[INFO] Processed 22000 images so far.\n",
      "[INFO] Processed 23000 images so far.\n",
      "[INFO] Processed 24000 images so far.\n",
      "[INFO] Processed 25000 images so far.\n",
      "[INFO] Processed 26000 images so far.\n",
      "[INFO] Processed 27000 images so far.\n",
      "[INFO] Processed 28000 images so far.\n",
      "[INFO] Processed 29000 images so far.\n",
      "[INFO] Processed 30000 images so far.\n",
      "[INFO] Processed 31000 images so far.\n",
      "[INFO] Processed 32000 images so far.\n",
      "[INFO] Processed 33000 images so far.\n",
      "[INFO] Processed 34000 images so far.\n",
      "Finished loading dataset.\n",
      "Training KNN\n",
      "KNN accuracy: 99.64978841383336\n"
     ]
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=7)\n",
    "\n",
    "def run_experiment():\n",
    "    # fix the random seed to make our experiments reproducible (since some algorithms use pseudorandom generators)\n",
    "    random_seed = 42  \n",
    "    random.seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "    \n",
    "    # Load dataset with extracted features\n",
    "    print('Loading dataset. This will take time ...')\n",
    "    features, labels = load_dataset()\n",
    "    print('Finished loading dataset.')\n",
    "    \n",
    "    # withhold some images that we will test the classifier on after training \n",
    "    train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size=0.2, random_state=random_seed)\n",
    "    print(\"Training KNN\")\n",
    "    \n",
    "    # Train the model only on the training features\n",
    "    model = knn\n",
    "\n",
    "    model.fit(train_features, train_labels)\n",
    "    \n",
    "    accuracy = model.score(test_features, test_labels)\n",
    "    \n",
    "    print(f\"KNN accuracy: {accuracy*100}\")\n",
    "\n",
    "run_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_and_filter_image(image, white_threshold=0.004):\n",
    "\n",
    "    height, width = image.shape\n",
    "    \n",
    "    # split horizontally into 4\n",
    "    part_width = width // 4   \n",
    "    accepted_parts = []\n",
    "    \n",
    "    # Loop through each part \n",
    "    for i in range(4):\n",
    "        \n",
    "        x_start = i * part_width\n",
    "        x_end = (i + 1) * part_width if i != 3 else width  # Ensure the last part includes the remainder\n",
    "        \n",
    "        part = image[:, x_start:x_end]  # Take the entire height for each part\n",
    "        # Count white pixels\n",
    "        num_white = np.sum(part == 255)\n",
    "        \n",
    "        # Calculate the fraction of white pixels in the part\n",
    "        white_fraction = num_white / (part.size)\n",
    "        # If the fraction of white pixels is above the threshold, keep the part\n",
    "        if white_fraction >= white_threshold:\n",
    "            accepted_parts.append(part)\n",
    "    \n",
    "    # Combine accepted parts into one image\n",
    "    if accepted_parts:\n",
    "        combined_image = np.concatenate(accepted_parts, axis=1) \n",
    "    else:\n",
    "        # If no parts meet the threshold, return an empty array\n",
    "        combined_image = np.zeros((height, width), dtype=np.uint8)\n",
    "    \n",
    "    return combined_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Character Segmentation\n",
    "\n",
    "def segment_characters(image):\n",
    "\n",
    "    # Adjust Aspect Ratio of plate\n",
    "    image = split_and_filter_image(image)\n",
    "\n",
    "    # Apply binary thresholding and dilation (to make characters clearer)\n",
    "    _, binary_image = cv.threshold(image,160, 255, cv.THRESH_BINARY_INV)\n",
    "    binary_image = cv.dilate(binary_image, np.ones((2,1)))\n",
    "\n",
    "    # Find contours of the characters\n",
    "    contours, _ = cv.findContours(binary_image, cv.RETR_TREE, cv.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    # Sort contours from left to right\n",
    "    sorted_contours = sorted(contours, key=lambda ctr: cv.boundingRect(ctr)[0])\n",
    "\n",
    "    # Initialize list to store segmented characters\n",
    "    segmented_characters = []\n",
    "    \n",
    "    width, height = binary_image.shape\n",
    "\n",
    "    # Loop through contours and extract individual characters\n",
    "    for i, contour in enumerate(sorted_contours):\n",
    "        x, y, w, h = cv.boundingRect(contour)\n",
    "        \n",
    "        # Filter out small contours\n",
    "        if w > width/8 and h > height/10 and w < width*7/8:\n",
    "            char_image = binary_image[y:y+h, x:x+w]\n",
    "            segmented_characters.append(char_image)\n",
    "            \n",
    "\n",
    "    return segmented_characters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifier\n",
    "def classifyChars(segmented):\n",
    "    detected =[]\n",
    "    # Loop over segmented characters and detect them using classifier\n",
    "    for i, char_image in enumerate(segmented):\n",
    "        features = extract_hog_features(char_image)\n",
    "        detected.append(knn.predict([features]))\n",
    "    \n",
    "    return detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "unordered_map = {}\n",
    "\n",
    "# assume parking for 1 second = 15 pounds\n",
    "def calculate_fee(plate_number):\n",
    "    current_time = time.time()  \n",
    "    \n",
    "    # Check if the string is already in the unordered map\n",
    "    if plate_number in unordered_map:\n",
    "        # Calculate the time difference\n",
    "        saved_time = unordered_map.pop(plate_number)\n",
    "        time_diff = current_time - saved_time\n",
    "        print(f\"Car with plate '{plate_number}'. exited after: {time_diff:.2f} seconds., Fee = {time_diff * 15:.2f}\")\n",
    "        return f\"{time_diff * 15:.2f}\"\n",
    "    else:\n",
    "        # Insert the string with the current time\n",
    "        unordered_map[plate_number] = current_time\n",
    "        print(f\"Car with plate '{plate_number}' just entered the parking\")\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_char_image(char_image):\n",
    "    # Resize to the input size of the model\n",
    "    char_image = cv.resize(char_image, (64, 64))\n",
    "\n",
    "    # Normalize pixel values\n",
    "    char_image = char_image / 255.0\n",
    "\n",
    "    # Add batch and channel dimensions\n",
    "    char_image = char_image.reshape(1, 64, 64, 1)\n",
    "\n",
    "    return char_image\n",
    "\n",
    "def predictOCR(segmented_characters):\n",
    "    # Load the trained model\n",
    "    model = load_model(\"chars74k_model.h5\")\n",
    "    print(\"Model loaded successfully!\")\n",
    "\n",
    "    # Initialize the predicted label\n",
    "    predicted_label = \"\"\n",
    "\n",
    "    # Iterate over each segmented character\n",
    "    for i, char_image in enumerate(segmented_characters):\n",
    "        # Preprocess the character image\n",
    "        char_image = preprocess_char_image(char_image)\n",
    "        # Predict the character\n",
    "        predictions = model.predict(char_image)\n",
    "        char_index = predictions.argmax()\n",
    "\n",
    "        # Convert the character index to a label (e.g., digit or letter)\n",
    "        if char_index < 10:\n",
    "            predicted_label += str(char_index)  # 0-9\n",
    "        else:\n",
    "            predicted_label += chr(char_index - 10 + ord('A'))  # A-Z\n",
    "\n",
    "    return predicted_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app 'PlateDetect'\n",
      " * Debug mode: off\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "from flask_cors import CORS\n",
    "import os\n",
    "import threading\n",
    "\n",
    "# API (Backend)\n",
    "\n",
    "server_thread = None\n",
    "stop_flag = threading.Event()\n",
    "\n",
    "app = Flask(\"PlateDetect\")\n",
    "CORS(app)\n",
    "\n",
    "@app.route('/extract_plate', methods=['POST'])\n",
    "def extract_plate():\n",
    "    try:\n",
    "        # Save Uploaded Image\n",
    "        image_file = request.files['image']\n",
    "        image_path = \"uploaded_image.jpg\"\n",
    "        image_file.save(image_path)\n",
    "        image_path = \"./uploaded_image.jpg\"\n",
    "\n",
    "        print(f\"Processing ${image_file}\")\n",
    "\n",
    "        # Preprocess Image\n",
    "        image = full_process_function(image_path)\n",
    "        show_images([image])\n",
    "        # Segment Characters\n",
    "        segmented_chars = segment_characters(image)\n",
    "        \n",
    "        # Detect Plate Number and Calculate fees (Classifier)\n",
    "        classified = classifyChars(segmented_chars)\n",
    "        print(classified)\n",
    "        plateNumber = ''.join([item[0] for item in classified])\n",
    "        print(classified)\n",
    "        fees = calculate_fee(plateNumber)\n",
    "\n",
    "        # Detect Plate Number and Calculate fees (OCR Deep Learning Model)\n",
    "        plateNumberOCR = predictOCR(segmented_chars)\n",
    "\n",
    "        # Clean Up Stored Image\n",
    "        os.remove(image_path)\n",
    "\n",
    "        return jsonify({\"plate\": plateNumber, \"fees\": fees, \"feesOCR\": fees, \"plateOCR\": plateNumberOCR})\n",
    "    except Exception as e:\n",
    "        return jsonify({\"error\": str(e)}), 500\n",
    "\n",
    "@app.route('/train_model', methods=['POST'])\n",
    "# Train Model Endpoint\n",
    "def train_model():\n",
    "    run_experiment()\n",
    "    return jsonify({\"message\": \"Model Training Done.\"})\n",
    "\n",
    "def run_flask():\n",
    "    while not stop_flag.is_set():\n",
    "        app.run(debug=False, use_reloader=False, port=5001)\n",
    "\n",
    "# Start Backend Thread\n",
    "server_thread = threading.Thread(target=run_flask)\n",
    "server_thread.daemon = True\n",
    "server_thread.start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Address already in use\n",
      "Port 5001 is in use by another program. Either identify and stop that program, or start the server with a different port.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing $<FileStorage: '3.jpg' ('image/jpeg')>\n",
      "Detected Impulses (Spikes): 0\n",
      "No salt and pepper noise detected.\n",
      "Mean R: 122.25, Mean G: 94.61, Mean B: 99.84\n",
      "The red channel is not dominant.\n",
      "No excessive red channel detected.\n",
      "Noise Standard Deviation: 8.22\n",
      "No significant Gaussian noise detected.\n",
      "Image does not contain significant Gaussian noise.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array(['F'], dtype='<U1'), array(['L'], dtype='<U1'), array(['F'], dtype='<U1'), array(['2'], dtype='<U1'), array(['4'], dtype='<U1')]\n",
      "[array(['F'], dtype='<U1'), array(['L'], dtype='<U1'), array(['F'], dtype='<U1'), array(['2'], dtype='<U1'), array(['4'], dtype='<U1')]\n",
      "Car with plate 'FLF24' just entered the parking\n",
      "Model loaded successfully!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [23/Dec/2024 03:29:28] \"POST /extract_plate HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Car with plate 'FLF24'. exited after: 0.93 seconds., Fee = 13.94\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:werkzeug:127.0.0.1 - - [23/Dec/2024 03:29:28] \"POST /extract_plate HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "# if server_thread is not None:\n",
    "#     stop_flag.set()  # Signal the thread to stop\n",
    "#     server_thread.join()  # Wait for the thread to finish\n",
    "#     print(\"Flask server stopped.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
