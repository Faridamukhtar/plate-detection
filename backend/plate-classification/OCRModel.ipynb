{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from commonfunctions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier  # MLP is an NN\n",
    "from sklearn import svm\n",
    "import numpy as np\n",
    "import argparse\n",
    "import cv2\n",
    "import os\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pytesseract\n",
    "\n",
    "import skimage\n",
    "import skimage.io as io\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import filters, feature\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.morphology import binary_erosion, binary_dilation, binary_closing,skeletonize, thin\n",
    "from skimage.measure import find_contours\n",
    "from skimage.draw import rectangle\n",
    "from scipy.ndimage import median_filter\n",
    "import cv2 as cv\n",
    "\n",
    "# Convolution:\n",
    "from scipy.signal import convolve2d\n",
    "from scipy import fftpack\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Path to the dataset\n",
    "DATASET_PATH = \"./English/Img\"  # Update this path to your Chars74K directory\n",
    "\n",
    "# Constants\n",
    "IMAGE_SIZE = (64, 64)  # Resize images to 64x64\n",
    "NUM_CLASSES = 36  # 26 letters + 10 digits\n",
    "\n",
    "def load_chars74k(dataset_path=DATASET_PATH):\n",
    "    \"\"\"Load the Chars74K dataset and preprocess images from both GoodImg and BadImg.\"\"\"\n",
    "    images = []\n",
    "    labels = []\n",
    "\n",
    "    # Define subfolders for GoodImg and BadImg\n",
    "    folders = ['GoodImg/Bmp', 'BadImag/Bmp']\n",
    "\n",
    "    for folder in folders:\n",
    "        folder_path = os.path.join(dataset_path, folder)\n",
    "\n",
    "        # Iterate over all character folders in GoodImg and BadImg\n",
    "        for label, char_folder in enumerate(sorted(os.listdir(folder_path))):\n",
    "            char_folder_path = os.path.join(folder_path, char_folder)\n",
    "            if not os.path.isdir(char_folder_path):\n",
    "                continue\n",
    "\n",
    "            # Iterate over all images in the character folder\n",
    "            for img_name in os.listdir(char_folder_path):\n",
    "                img_path = os.path.join(char_folder_path, img_name)\n",
    "                img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "                # Resize and normalize the image\n",
    "                img = cv2.resize(img, IMAGE_SIZE)\n",
    "                img = img / 255.0\n",
    "                images.append(img)\n",
    "                labels.append(label)\n",
    "\n",
    "    # Convert to numpy arrays\n",
    "    images = np.array(images).reshape(-1, IMAGE_SIZE[0], IMAGE_SIZE[1], 1)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    return images, labels\n",
    "\n",
    "\n",
    "def build_cnn_model(input_shape=(64, 64, 1), num_classes=NUM_CLASSES):\n",
    "    \"\"\"Build and compile a CNN model.\"\"\"\n",
    "    model = Sequential([\n",
    "        Conv2D(32, (3, 3), activation=\"relu\", input_shape=input_shape),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Conv2D(64, (3, 3), activation=\"relu\"),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Conv2D(128, (3, 3), activation=\"relu\"),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Flatten(),\n",
    "        Dense(256, activation=\"relu\"),\n",
    "        Dropout(0.5),\n",
    "        Dense(num_classes, activation=\"softmax\"),\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "\n",
    "def runOCR():\n",
    "    # Load and preprocess the dataset\n",
    "    print(\"Loading dataset...\")\n",
    "    images, labels = load_chars74k()\n",
    "    print(f\"Loaded {len(images)} images and {len(labels)} labels.\")\n",
    "\n",
    "    # One-hot encode labels\n",
    "    labels = to_categorical(labels, NUM_CLASSES)\n",
    "\n",
    "    # Split into training and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Build the CNN model\n",
    "    print(\"Building model...\")\n",
    "    model = build_cnn_model()\n",
    "\n",
    "    # Train the model\n",
    "    print(\"Training model...\")\n",
    "    history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=32)\n",
    "\n",
    "    # Evaluate the model\n",
    "    print(\"Evaluating model...\")\n",
    "    test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "    print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n",
    "\n",
    "    # Save the model\n",
    "    model.save(\"chars74k_model.h5\")\n",
    "    print(\"Model saved as chars74k_model.h5\")\n",
    "\n",
    "\n",
    "runOCR()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
