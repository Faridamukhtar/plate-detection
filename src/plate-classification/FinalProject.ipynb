{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from commonfunctions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier  # MLP is an NN\n",
    "from sklearn import svm\n",
    "import numpy as np\n",
    "import argparse\n",
    "import cv2\n",
    "import os\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pytesseract\n",
    "\n",
    "import skimage\n",
    "import skimage.io as io\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import filters, feature\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.morphology import binary_erosion, binary_dilation, binary_closing,skeletonize, thin\n",
    "from skimage.measure import find_contours\n",
    "from skimage.draw import rectangle\n",
    "from scipy.ndimage import median_filter\n",
    "import cv2 as cv\n",
    "\n",
    "# Convolution:\n",
    "from scipy.signal import convolve2d\n",
    "from scipy import fftpack\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import List\n",
    "\n",
    "\n",
    "def get_largest_contour_list(contour_lists: List[List]) -> List[List]:\n",
    "    \"\"\"\n",
    "    Returns a list containing the contour list with the largest number of contours.\n",
    "    \n",
    "    Parameters:\n",
    "    - contour_lists (List[List]): A list containing multiple lists of contours.\n",
    "\n",
    "    Returns:\n",
    "    - List[List]: A list containing the contour list with the maximum number of contours.\n",
    "    \"\"\"\n",
    "    # Handle empty input\n",
    "    if not contour_lists:\n",
    "        return []\n",
    "    if not contour_lists:\n",
    "        raise ValueError(\"Input must contain at least one list of contours.\")\n",
    "    \n",
    "    # Find the list with the maximum number of contours\n",
    "    largest_contour_list = max(contour_lists, key=len)\n",
    "    \n",
    "    # Return as a list containing only the largest contour list\n",
    "    return [largest_contour_list]\n",
    "\n",
    "# Function for bright RGB image enhancement\n",
    "def enhance_bright_image_rgb(image):\n",
    "    enhanced_image = image.copy()\n",
    "    for channel in range(3):  # Process R, G, B channels\n",
    "        for y in range(image.shape[0]):\n",
    "            for x in range(image.shape[1]):\n",
    "                pixel_value = image[y, x, channel]\n",
    "                if 200 <= pixel_value <= 230:\n",
    "                    # Scale pixel value between 240 and 255\n",
    "                    enhanced_image[y, x, channel] = 240 + (pixel_value - 200) * (255 - 240) // (230 - 200)\n",
    "                elif pixel_value > 230:\n",
    "                    # Keep values above 230 as they are\n",
    "                    enhanced_image[y, x, channel] = 255\n",
    "                else:\n",
    "                    # Keep values below 200 unchanged\n",
    "                    enhanced_image[y, x, channel] = pixel_value\n",
    "    return enhanced_image\n",
    "\n",
    "# Function for dark RGB image enhancement\n",
    "def enhance_dark_image_rgb(image):\n",
    "    enhanced_image = image.copy()\n",
    "    for channel in range(3):  # Process R, G, B channels\n",
    "        for y in range(image.shape[0]):\n",
    "            for x in range(image.shape[1]):\n",
    "                pixel_value = image[y, x, channel]\n",
    "                if 130 <= pixel_value <= 200:\n",
    "                    # Scale pixel value between 230 and 255\n",
    "                    enhanced_image[y, x, channel] = 230 + (pixel_value - 130) * (255 - 230) // (200 - 130)\n",
    "                elif pixel_value > 200:\n",
    "                    # Keep values above 200 as they are\n",
    "                    enhanced_image[y, x, channel] = 255\n",
    "                else:\n",
    "                    # Keep values below 130 unchanged\n",
    "                    enhanced_image[y, x, channel] = pixel_value\n",
    "    return enhanced_image\n",
    "\n",
    "def enhance_very_dark_image_rgb(image):\n",
    "    enhanced_image = image.copy()\n",
    "    for channel in range(3):  # Process R, G, B channels\n",
    "        for y in range(image.shape[0]):\n",
    "            for x in range(image.shape[1]):\n",
    "                pixel_value = image[y, x, channel]\n",
    "                if 90 <= pixel_value <= 150:\n",
    "                    # Scale pixel value between 230 and 250\n",
    "                    enhanced_image[y, x, channel] = 230 + (pixel_value - 90) * (250 - 230) // (150 - 90)\n",
    "                elif pixel_value > 150:\n",
    "                    # Keep values above 150 as they are\n",
    "                    enhanced_image[y, x, channel] = pixel_value\n",
    "                else:\n",
    "                    # Keep values below 90 unchanged\n",
    "                    enhanced_image[y, x, channel] = pixel_value\n",
    "    return enhanced_image\n",
    "\n",
    "def enhance_bright_image(image):\n",
    "    enhanced_image = image.copy()\n",
    "    for y in range(image.shape[0]):\n",
    "        for x in range(image.shape[1]):\n",
    "            pixel_value = image[y, x]\n",
    "            if 200 <= pixel_value <= 230:\n",
    "                # Scale pixel value between 240 and 255\n",
    "                enhanced_image[y, x] = 240 + (pixel_value - 200) * (255 - 240) // (230 - 200)\n",
    "            elif pixel_value > 230:\n",
    "                # Keep values above 230 as they are\n",
    "                enhanced_image[y, x] = 255\n",
    "            else:\n",
    "                # Keep values below 200 unchanged\n",
    "                enhanced_image[y, x] = pixel_value\n",
    "    return enhanced_image\n",
    "\n",
    "def enhance_dark_image(image):\n",
    "    enhanced_image = image.copy()\n",
    "    for y in range(image.shape[0]):\n",
    "        for x in range(image.shape[1]):\n",
    "            pixel_value = image[y, x]\n",
    "            if 130 <= pixel_value <= 200:\n",
    "                # Scale pixel value between 230 and 255\n",
    "                enhanced_image[y, x] = 230 + (pixel_value - 130) * (255 - 230) // (200 - 130)\n",
    "            elif pixel_value > 200:\n",
    "                # Keep values above 200 as they are\n",
    "                enhanced_image[y, x] = 255\n",
    "            else:\n",
    "                # Keep values below 130 unchanged\n",
    "                enhanced_image[y, x] = pixel_value\n",
    "    return enhanced_image\n",
    "\n",
    "def calculate_percentage_of_ones(binary_img):\n",
    "    \n",
    "\n",
    "    # Calculate the number of 1's\n",
    "    num_ones = np.count_nonzero(binary_img)\n",
    "\n",
    "    # Calculate the total number of pixels\n",
    "    total_pixels = binary_img.size\n",
    "\n",
    "    # Calculate the percentage of 1's\n",
    "    percentage_of_ones = (num_ones / total_pixels) * 100\n",
    "\n",
    "    return percentage_of_ones\n",
    "\n",
    "\n",
    "\n",
    "# Function for enhancing both dark and bright RGB image enhancement\n",
    "def enhance_image_rgb(image):\n",
    "    enhanced_image = image.copy()\n",
    "\n",
    "    for channel in range(3):  # Process R, G, B channels\n",
    "        for y in range(image.shape[0]):\n",
    "            for x in range(image.shape[1]):\n",
    "                pixel_value = image[y, x, channel]\n",
    "                \n",
    "                # Enhance dark to very dark pixel values (90 to 200)\n",
    "                if 90 <= pixel_value <= 200:\n",
    "                    if pixel_value <= 130:\n",
    "                        # Scale very dark pixels between 90 and 150 to 230 to 250\n",
    "                        enhanced_image[y, x, channel] = 230 + (pixel_value - 90) * (250 - 230) // (150 - 90)\n",
    "                    elif pixel_value <= 200:\n",
    "                        # Scale dark pixels between 130 and 200 to 230 to 255\n",
    "                        enhanced_image[y, x, channel] = 230 + (pixel_value - 130) * (255 - 230) // (200 - 130)\n",
    "                # Enhance bright pixels (200 to 230 range)\n",
    "                elif 200 <= pixel_value <= 230:\n",
    "                    # Scale bright pixels between 200 and 230 to 240 to 255\n",
    "                    enhanced_image[y, x, channel] = 240 + (pixel_value - 200) * (255 - 240) // (230 - 200)\n",
    "                elif pixel_value > 230:\n",
    "                    # Keep values above 230 as they are (already maximized)\n",
    "                    enhanced_image[y, x, channel] = 255\n",
    "                else:\n",
    "                    # Keep values below 90 unchanged (dark/black areas)\n",
    "                    enhanced_image[y, x, channel] = pixel_value\n",
    "\n",
    "    return enhanced_image\n",
    "\n",
    "def enhance_image(image):\n",
    "    enhanced_image = image.copy()\n",
    "\n",
    "    for y in range(image.shape[0]):\n",
    "        for x in range(image.shape[1]):\n",
    "            pixel_value = image[y, x]\n",
    "                \n",
    "            # Enhance dark to very dark pixel values (90 to 200)\n",
    "            if 90 <= pixel_value <= 200:\n",
    "                if pixel_value <= 130:\n",
    "                    # Scale very dark pixels between 90 and 150 to 230 to 250\n",
    "                    enhanced_image[y, x] = 230 + (pixel_value - 90) * (250 - 230) // (150 - 90)\n",
    "                elif pixel_value <= 200:\n",
    "                    # Scale dark pixels between 130 and 200 to 230 to 255\n",
    "                    enhanced_image[y, x] = 230 + (pixel_value - 130) * (255 - 230) // (200 - 130)\n",
    "            # Enhance bright pixels (200 to 230 range)\n",
    "            elif 200 <= pixel_value <= 230:\n",
    "                # Scale bright pixels between 200 and 230 to 240 to 255\n",
    "                enhanced_image[y, x] = 240 + (pixel_value - 200) * (255 - 240) // (230 - 200)\n",
    "            elif pixel_value > 230:\n",
    "                # Keep values above 230 as they are (already maximized)\n",
    "                enhanced_image[y, x] = 255\n",
    "            else:\n",
    "                # Keep values below 90 unchanged (dark/black areas)\n",
    "                enhanced_image[y, x] = pixel_value\n",
    "\n",
    "    return enhanced_image\n",
    "\n",
    "from scipy.signal import find_peaks, peak_widths\n",
    "def has_salt_and_pepper_noise(image):\n",
    "    # Convert to grayscale if needed\n",
    "    if len(image.shape) == 3:\n",
    "        image = cv.cvtColor(image, cv.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Flatten the image for histogram analysis\n",
    "    flattened_image = image.flatten()\n",
    "    hist, bins = np.histogram(flattened_image, bins=256, range=(0, 255))\n",
    "\n",
    "    # Detect peaks\n",
    "    peaks, properties = find_peaks(hist, prominence=0.1 * np.max(hist))  # Prominent peaks\n",
    "    widths = peak_widths(hist, peaks, rel_height=0.5)[0]  # Measure widths at half prominence\n",
    "\n",
    "    # Filter peaks that are narrow (spikes)\n",
    "    spike_indices = [i for i, width in enumerate(widths) if width <= 2]  # Adjust width threshold\n",
    "    spikes = peaks[spike_indices]\n",
    "\n",
    "    # Count spikes\n",
    "    num_spikes = len(spikes)\n",
    "    print(f\"Detected Impulses (Spikes): {num_spikes}\")\n",
    "\n",
    "    # Decision based on spike count\n",
    "    if num_spikes >= 5:  # Adjust threshold as needed\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "# checks whether the red channel is dominating \n",
    "def detect_dominant_red_channel(image, red_threshold=1.3):\n",
    "    # Ensure the image has 3 channels (RGB)\n",
    "    if image.shape[-1] != 3:\n",
    "        raise ValueError(\"Image must be an RGB image\")\n",
    "\n",
    "    # Split the image into R, G, B channels\n",
    "    R = image[:, :, 0]\n",
    "    G = image[:, :, 1]\n",
    "    B = image[:, :, 2]\n",
    "\n",
    "    # Calculate the mean intensity of each channel\n",
    "    mean_R = np.mean(R)\n",
    "    mean_G = np.mean(G)\n",
    "    mean_B = np.mean(B)\n",
    "\n",
    "    print(f\"Mean R: {mean_R:.2f}, Mean G: {mean_G:.2f}, Mean B: {mean_B:.2f}\")\n",
    "\n",
    "    # Check if the red channel is dominant compared to green and blue\n",
    "    if mean_R > red_threshold * mean_G and mean_R > red_threshold * mean_B:\n",
    "        print(\"The red channel is dominant.\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"The red channel is not dominant.\")\n",
    "        return False\n",
    "    \n",
    "# function used to equalize the red channel\n",
    "def equalizeredchannel(img):\n",
    "    img = img.astype(np.float32)\n",
    "    \n",
    "    red_channel = img[:, :, 0]\n",
    "    green_channel = img[:, :, 1]\n",
    "    blue_channel = img[:, :, 2]\n",
    "\n",
    "    original_red = np.clip(red_channel / 1.5, 0, 255).astype(np.uint8)\n",
    "    \n",
    "    equalized_img = np.stack((original_red, green_channel, blue_channel), axis=2).astype(np.uint8)\n",
    "    return equalized_img\n",
    "    \n",
    "# checks if the image contains additive noise or not\n",
    "def detect_gaussian_noise(image, threshold_std=10):\n",
    "    #  Convert to grayscale if needed\n",
    "    if len(image.shape) == 3:\n",
    "        gray = cv.cvtColor(image, cv.COLOR_BGR2GRAY)\n",
    "    else:\n",
    "        gray = image\n",
    "\n",
    "    # Smooth the image using Gaussian Blur\n",
    "    blurred = cv.GaussianBlur(gray, (5, 5), 1.0)\n",
    "\n",
    "    # Subtract the blurred image from the original image to isolate noise\n",
    "    noise = gray.astype(np.float32) - blurred.astype(np.float32)\n",
    "\n",
    "    # Calculate the standard deviation of the noise\n",
    "    noise_std = np.std(noise)\n",
    "    print(f\"Noise Standard Deviation: {noise_std:.2f}\")\n",
    "\n",
    "    # Threshold to classify noise\n",
    "    if noise_std > threshold_std:\n",
    "        print(\"Gaussian noise detected.\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"No significant Gaussian noise detected.\")\n",
    "        return False\n",
    "\n",
    "def apply_mask(image, mask):\n",
    "    \"\"\"\n",
    "    Apply a binary mask to an image.\n",
    "    \n",
    "    Parameters:\n",
    "    - image: The input image (numpy array).\n",
    "    - mask: The binary mask (numpy array), where `1` indicates that the pixel in the image should be kept, \n",
    "    and `0` means the pixel should be masked out.\n",
    "    \n",
    "    Returns:\n",
    "    - result: The image after the mask has been applied.\n",
    "    \"\"\"\n",
    "    # Ensure the mask is binary (0 or 1) and has the same shape as the image\n",
    "    mask = mask.astype(np.uint8)  # Convert mask to uint8 for safety\n",
    "    if mask.shape != image.shape:\n",
    "        raise ValueError(\"The image and mask must have the same shape\")\n",
    "\n",
    "    # Apply the mask to the image: element-wise multiplication\n",
    "    result = cv.bitwise_and(image, mask)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def process_image(imageIn):\n",
    "#     # 1- Get the required RGB color (Using a color picker)\n",
    "\n",
    "#     img = imageIn\n",
    "\n",
    "#     if img.shape[0] < 800 and img.shape[1] < 600:\n",
    "#         img = cv.resize(img, (800, 600))\n",
    "#     imgg = img\n",
    "#     if has_salt_and_pepper_noise(img):\n",
    "#         print(\"Salt and pepper noise detected!\")\n",
    "#         img = median_filter(img, size = 3)\n",
    "#     else:\n",
    "#         print(\"No salt and pepper noise detected.\")\n",
    "\n",
    "#     # Assuming img is your input image\n",
    "#     if img.dtype == np.float64:\n",
    "#         # Convert the image to uint8\n",
    "#         img = img.astype(np.uint8)\n",
    "\n",
    "#         # Or use .astype(np.uint8) directly if the values are already in [0, 255]\n",
    "\n",
    "#     img = cv.cvtColor(img, cv.COLOR_BGR2RGB)\n",
    "#     image = np.copy(img)\n",
    "#     if img.shape[2] == 4:\n",
    "#         image = rgba2rgb(image)\n",
    "#     if detect_dominant_red_channel(image):\n",
    "#         print(\"Excessive red channel detected.\")\n",
    "#         img = equalizeredchannel(img)\n",
    "#     else:\n",
    "#         print(\"No excessive red channel detected.\")\n",
    "\n",
    "#     if detect_gaussian_noise(img, threshold_std=15):\n",
    "#         print(\"Image contains additive Gaussian noise.\")\n",
    "#         img = gaussian(img, sigma = 1.3)\n",
    "#     else:\n",
    "#         print(\"Image does not contain significant Gaussian noise.\")\n",
    "\n",
    "#     if len(img.shape) == 3:\n",
    "#         height , width , channel = img.shape\n",
    "#     elif len(img.shape) == 2:\n",
    "#         height , width  = img.shape\n",
    "\n",
    "#     gray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
    "\n",
    "#     img = enhance_image_rgb(img)\n",
    "\n",
    "#     TragetColor = np.array([240/255,240/255,240/255]) \n",
    "\n",
    "#     # 2- Read image\n",
    "\n",
    "#     img = img.astype(float) / 255\n",
    "\n",
    "#     # 3- Extract R, G, and B channels (as float)\n",
    "#     RedChannel = img[:,:,0]\n",
    "#     GreenChannel = img[:,:,1]\n",
    "#     BlueChannel = img[:,:,2]\n",
    "\n",
    "#     # 4- Calculate differences FOR EACH CHANNEL (between the image and the required pixel value)\n",
    "#     DiffRed = np.abs(RedChannel - TragetColor[0])\n",
    "#     DiffGreen = np.abs(GreenChannel - TragetColor[1])\n",
    "#     DiffBlue = np.abs(BlueChannel - TragetColor[2])\n",
    "\n",
    "#     # 5- Calculate overall distance from the given RGB color (use Euclidean distance)\n",
    "#     Distance = np.sqrt(DiffRed**2 + DiffGreen**2 + DiffBlue**2)\n",
    "\n",
    "#     # 6- Create a mask by thresholding the differences\n",
    "#     Threshold = 0.1\n",
    "#     Mask = np.where(Distance < Threshold , True,False)\n",
    "\n",
    "#     # 7- Replace the pixels of the mask with the new color (R=230, G=90, B=40)\n",
    "#     ReplacedColor = np.array([230/255, 90/255, 40/255])\n",
    "\n",
    "#     # Create a copy of the image to apply the changes\n",
    "#     modified_img = np.copy(img)\n",
    "\n",
    "#     struct_elem = np.ones((2, 3))\n",
    "#     img_dilated1 = binary_dilation(Mask, struct_elem).astype(img.dtype)\n",
    "\n",
    "#     struct_elem = np.ones((12, 4))\n",
    "#     img_closed = binary_erosion(img_dilated1, struct_elem).astype(img.dtype)\n",
    "\n",
    "#     struct_elem = np.ones((70, 180))\n",
    "#     img_dilated = binary_dilation(img_closed, struct_elem).astype(img.dtype)\n",
    "\n",
    "#     percentageOfOnes   = calculate_percentage_of_ones(img_dilated )\n",
    "#     # Apply the new color where the mask is True\n",
    "#     modified_img[Mask] = ReplacedColor\n",
    "\n",
    "#     # Convert to binary image using a simple threshold\n",
    "#     ret, binary_img = cv.threshold(gray, 210, 255, cv.THRESH_BINARY)\n",
    "\n",
    "#     struct_elem = np.ones((50,50))\n",
    "#     img_Dilatedd_adaptive = binary_dilation(binary_img, struct_elem).astype(img.dtype)\n",
    "\n",
    "#     # Optionally, visualize contours on the labeled image\n",
    "#     contours, _ = cv.findContours(binary_img, cv.RETR_CCOMP, cv.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "#     masked_img_dilated = np.zeros_like(gray)  # Create an empty array with the same shape as gray\n",
    "#     masked_img_dilated[img_dilated >= 1] = gray[img_dilated >= 1]  # Retain the grayscale values where mask is 255\n",
    "\n",
    "#     masked_img_dilated_adaptivaly = np.zeros_like(gray)  # Create an empty array with the same shape as gray\n",
    "#     masked_img_dilated_adaptivaly[img_Dilatedd_adaptive >= 1] = gray[img_Dilatedd_adaptive >= 1]  # Retain the grayscale values where mask is 255\n",
    "\n",
    "\n",
    "#     if percentageOfOnes >= 90 and percentageOfOnes <91:\n",
    "#         output_img = masked_img_dilated_adaptivaly\n",
    "#     elif percentageOfOnes >=99:\n",
    "#         output_img = masked_img_dilated_adaptivaly\n",
    "#     else:\n",
    "#         output_img = masked_img_dilated\n",
    "\n",
    "#     blurred_img = cv.GaussianBlur(output_img , ksize=(5,5) , sigmaX=0)\n",
    "#     thresholded_img = cv.adaptiveThreshold(blurred_img , maxValue=255.0 , adaptiveMethod=cv.ADAPTIVE_THRESH_MEAN_C\n",
    "#                                             ,thresholdType=cv.THRESH_BINARY_INV, blockSize=19,C=9)\n",
    "\n",
    "\n",
    "#     contours ,_ = cv.findContours(thresholded_img , mode = cv.RETR_LIST , method=cv.CHAIN_APPROX_SIMPLE)\n",
    "#     temp_result = np.zeros((height,width,channel),dtype=np.uint8)\n",
    "#     cv.drawContours(temp_result , contours=contours , contourIdx=-1 , color=(255,255,255))\n",
    "\n",
    "#     max_diag_multiplyer = 4.1\n",
    "#     max_angle_diff = 25.0\n",
    "#     max_area_diff = 1.47\n",
    "#     max_width_diff = 1.5\n",
    "#     max_height_diff = 0.18\n",
    "#     min_n_matched = 4\n",
    "\n",
    "#     def find_chars(contours_list):\n",
    "#         matched_result_idx = []\n",
    "\n",
    "#         for d1 in contours_list:\n",
    "#             matched_contour_idx = []\n",
    "#             for d2 in contours_list:\n",
    "#                 if d1['idx'] == d2['idx']:\n",
    "#                     continue\n",
    "\n",
    "#                 dx = abs(d1['cx'] - d2['cx'])\n",
    "#                 dy = abs(d1['cy'] - d2['cy'])\n",
    "\n",
    "#                 diagonal_length = np.sqrt(d1['w']**2 + d1['h']**2)\n",
    "#                 distance = np.linalg.norm(np.array([d1['cx'], d1['cy']]) - np.array([d2['cx'], d2['cy']]))\n",
    "\n",
    "#                 angle_diff = 90 if dx == 0 else np.degrees(np.arctan(dy / dx))\n",
    "                \n",
    "#                 area_diff = abs((d1['w'] * d1['h'] - d2['w'] * d2['h']) / (d1['w'] * d1['h']))\n",
    "#                 width_diff = abs((d1['w'] - d2['w']) / d1['w'])\n",
    "#                 height_diff = abs((d1['h'] - d2['h']) / d1['h'])\n",
    "\n",
    "#                 if distance < diagonal_length * max_diag_multiplyer and angle_diff < max_angle_diff \\\n",
    "#                     and area_diff < max_area_diff and width_diff < max_width_diff and height_diff < max_height_diff:\n",
    "#                     matched_contour_idx.append(d2['idx'])\n",
    "\n",
    "#             if len(matched_contour_idx) < min_n_matched:\n",
    "#                 continue\n",
    "\n",
    "#             matched_result_idx.append([d1['idx']] + matched_contour_idx)\n",
    "\n",
    "#             unmatched_contour_idx = [d4['idx'] for d4 in contours_list if d4['idx'] not in matched_contour_idx]\n",
    "\n",
    "#             if not unmatched_contour_idx:\n",
    "#                 break\n",
    "\n",
    "#             unmatched_contour = np.array([d for d in contours_list if d['idx'] in unmatched_contour_idx])\n",
    "#             recursive_contour_list = find_chars(unmatched_contour)\n",
    "\n",
    "#             for idx in recursive_contour_list:\n",
    "#                 matched_result_idx.append(idx)\n",
    "\n",
    "#             break\n",
    "\n",
    "#         return matched_result_idx\n",
    "\n",
    "#     plate_width_padding = 0.6\n",
    "#     plate_height_padding = 1.5\n",
    "#     min_plate_ratio =1\n",
    "#     max_plate_ratio = 2\n",
    "\n",
    "\n",
    "#     plate_imgs = []\n",
    "#     plate_infos = []\n",
    "\n",
    "#     for i , matched_chars in enumerate(matched_result):\n",
    "#         sorted_chars = sorted(matched_chars , key=lambda x : x['cx'])\n",
    "\n",
    "#         plate_cx = (sorted_chars[0]['cx'] + sorted_chars[-1]['cx']) /2\n",
    "#         plate_cy = (sorted_chars[0]['cy'] + sorted_chars[-1]['cy']) /2\n",
    "\n",
    "#         plate_width = (sorted_chars[-1]['x'] + sorted_chars[-1]['w'] - sorted_chars[0]['w'] ) * plate_width_padding\n",
    "\n",
    "#         sum_height = 0\n",
    "#         cropped_imgs = []\n",
    "#         for d in sorted_chars:\n",
    "#             sum_height += d['h']\n",
    "\n",
    "\n",
    "#         plate_height = int(sum_height / len(sorted_chars) * plate_height_padding)\n",
    "\n",
    "#         triangle_height = sorted_chars[-1]['cy'] - sorted_chars[0]['cy']\n",
    "#         triangle_hypotenus = np.linalg.norm(\n",
    "#             np.array([sorted_chars[0]['cx'] , sorted_chars[0]['cy']]) -\n",
    "#             np.array([sorted_chars[-1]['cx'] , sorted_chars[-1]['cy']])\n",
    "#         )\n",
    "\n",
    "#         angle = np.degrees(np.arcsin(triangle_height / triangle_hypotenus))\n",
    "\n",
    "#         rotation_matrix = cv.getRotationMatrix2D(center=(plate_cx , plate_cy) , angle=angle , scale=1.0)\n",
    "\n",
    "#         img_rotated = cv.warpAffine(thresholded_img , M=rotation_matrix , dsize=(width, height))\n",
    "\n",
    "#         img_cropped = cv.getRectSubPix(\n",
    "#             img_rotated,\n",
    "#             patchSize=(int(plate_width) , int(plate_height)),\n",
    "#             center = (int(plate_cx) , int(plate_cy))\n",
    "#         )\n",
    "\n",
    "#         if img_cropped.shape[1] / img_cropped.shape[0] < min_plate_ratio or min_plate_ratio < img_cropped.shape[1] / img_cropped.shape[0] <  max_plate_ratio:\n",
    "#             continue\n",
    "\n",
    "\n",
    "#         plate_imgs.append(img_cropped)\n",
    "        \n",
    "#         plate_infos.append({\n",
    "#             'x':int(plate_cx - plate_width /2),\n",
    "#             'y':int(plate_cy - plate_height /2),\n",
    "#             'w':int(plate_width),\n",
    "#             'h':int(plate_height)\n",
    "#         })\n",
    "\n",
    "#     plate_width_padding = 0.6\n",
    "#     plate_height_padding = 1.5\n",
    "#     min_plate_ratio =1\n",
    "#     max_plate_ratio = 2\n",
    "\n",
    "\n",
    "#     plate_imgs = []\n",
    "#     plate_infos = []\n",
    "\n",
    "#     for i , matched_chars in enumerate(matched_result):\n",
    "#         sorted_chars = sorted(matched_chars , key=lambda x : x['cx'])\n",
    "\n",
    "#         plate_cx = (sorted_chars[0]['cx'] + sorted_chars[-1]['cx']) /2\n",
    "#         plate_cy = (sorted_chars[0]['cy'] + sorted_chars[-1]['cy']) /2\n",
    "\n",
    "#         plate_width = (sorted_chars[-1]['x'] + sorted_chars[-1]['w'] - sorted_chars[0]['w'] ) * plate_width_padding\n",
    "\n",
    "#         sum_height = 0\n",
    "#         cropped_imgs = []\n",
    "#         for d in sorted_chars:\n",
    "#             sum_height += d['h']\n",
    "\n",
    "\n",
    "#         plate_height = int(sum_height / len(sorted_chars) * plate_height_padding)\n",
    "\n",
    "#         triangle_height = sorted_chars[-1]['cy'] - sorted_chars[0]['cy']\n",
    "#         triangle_hypotenus = np.linalg.norm(\n",
    "#             np.array([sorted_chars[0]['cx'] , sorted_chars[0]['cy']]) -\n",
    "#             np.array([sorted_chars[-1]['cx'] , sorted_chars[-1]['cy']])\n",
    "#         )\n",
    "\n",
    "#         angle = np.degrees(np.arcsin(triangle_height / triangle_hypotenus))\n",
    "\n",
    "#         rotation_matrix = cv.getRotationMatrix2D(center=(plate_cx , plate_cy) , angle=angle , scale=1.0)\n",
    "\n",
    "#         img_rotated = cv.warpAffine(thresholded_img , M=rotation_matrix , dsize=(width, height))\n",
    "\n",
    "#         img_cropped = cv.getRectSubPix(\n",
    "#             img_rotated,\n",
    "#             patchSize=(int(plate_width) , int(plate_height)),\n",
    "#             center = (int(plate_cx) , int(plate_cy))\n",
    "#         )\n",
    "\n",
    "#         if img_cropped.shape[1] / img_cropped.shape[0] < min_plate_ratio or min_plate_ratio < img_cropped.shape[1] / img_cropped.shape[0] <  max_plate_ratio:\n",
    "#             continue\n",
    "\n",
    "\n",
    "#         plate_imgs.append(img_cropped)\n",
    "        \n",
    "#         plate_infos.append({\n",
    "#             'x':int(plate_cx - plate_width /2),\n",
    "#             'y':int(plate_cy - plate_height /2),\n",
    "#             'w':int(plate_width),\n",
    "#             'h':int(plate_height)\n",
    "#         })\n",
    "\n",
    "\n",
    "#     cropped = img_cropped\n",
    "\n",
    "#     contours ,_ = cv.findContours(img_cropped , mode = cv.RETR_LIST , method=cv.CHAIN_APPROX_SIMPLE)\n",
    "#     temp_result = np.zeros((img_cropped.shape[0],img_cropped.shape[1]),dtype=np.uint8)\n",
    "#     cv.drawContours(temp_result , contours=contours , contourIdx=-1 , color=(255,255,255))\n",
    "\n",
    "#     temp_result = np.zeros((img_cropped.shape[0],img_cropped.shape[1]),dtype=np.uint8)\n",
    "#     contours_dict=[]\n",
    "\n",
    "#     for contour in contours:\n",
    "#         x,y,w,h = cv.boundingRect(contour)\n",
    "#         cv.rectangle(temp_result , pt1=(x,y),pt2=(x+w,y+h) , color=(255,255,255) , thickness=1)\n",
    "#         #insert the rectangles into the dict\n",
    "#         contours_dict.append({'contour' : contour , 'x':x,'y':y,'w':w,'h':h ,'cx':x+(w/2),'cy': y+(h/2)})\n",
    "\n",
    "#     min_area = 149\n",
    "#     max_area = 2000\n",
    "#     min_width , min_height = 1,5\n",
    "#     min_ratio , max_ratio = 0.25 , 1.62\n",
    "\n",
    "#     possible_contours = []\n",
    "\n",
    "#     cnt = 0\n",
    "\n",
    "#     for d in contours_dict:\n",
    "#         area = d['w'] * d['h']\n",
    "#         ratio = d['w'] / d['h']\n",
    "\n",
    "#         if min_area<area <max_area  and d['w'] > min_width and d['h'] > min_height and min_ratio < ratio <max_ratio:\n",
    "#             d['idx'] = cnt\n",
    "#             cnt +=1\n",
    "#             possible_contours.append(d)\n",
    "\n",
    "#     temp_result = np.zeros((img_cropped.shape[0],img_cropped.shape[1]),dtype=np.uint8)\n",
    "#     for d in possible_contours:\n",
    "#         cv.rectangle(temp_result , pt1=(d['x'],d['y']) , pt2=(d['x']+d['w'] , d['y']+d['h']) , color=(255,255,255) , thickness=2)\n",
    "\n",
    "#     max_diag_multiplyer = 8\n",
    "#     max_angle_diff = 25.0\n",
    "#     max_area_diff = 10\n",
    "#     max_width_diff = 5\n",
    "#     max_height_diff = 0.2  #changed to 0.3 to let image 23 to work and also 31\n",
    "#     min_n_matched = 2\n",
    "#     def find_chars(contours_list):\n",
    "#         matched_result_idx = []\n",
    "\n",
    "#         for d1 in contours_list:\n",
    "#             matched_contour_idx = []\n",
    "#             for d2 in contours_list:\n",
    "#                 if d1['idx'] == d2['idx']:\n",
    "#                     continue\n",
    "\n",
    "#                 dx = abs(d1['cx'] - d2['cx'])\n",
    "#                 dy = abs(d1['cy'] - d2['cy'])\n",
    "\n",
    "#                 diagonal_length = np.sqrt(d1['w']**2 + d1['h']**2)\n",
    "#                 distance = np.linalg.norm(np.array([d1['cx'], d1['cy']]) - np.array([d2['cx'], d2['cy']]))\n",
    "\n",
    "#                 angle_diff = 90 if dx == 0 else np.degrees(np.arctan(dy / dx))\n",
    "                \n",
    "#                 area_diff = abs((d1['w'] * d1['h'] - d2['w'] * d2['h']) / (d1['w'] * d1['h']))\n",
    "#                 width_diff = abs((d1['w'] - d2['w']) / d1['w'])\n",
    "#                 height_diff = abs((d1['h'] - d2['h']) / d1['h'])\n",
    "\n",
    "#                 if distance < diagonal_length * max_diag_multiplyer and angle_diff < max_angle_diff \\\n",
    "#                     and area_diff < max_area_diff and width_diff < max_width_diff and height_diff < max_height_diff:\n",
    "#                     matched_contour_idx.append(d2['idx'])\n",
    "\n",
    "#             if len(matched_contour_idx) < min_n_matched:\n",
    "#                 continue\n",
    "\n",
    "#             matched_result_idx.append([d1['idx']] + matched_contour_idx)\n",
    "\n",
    "#             unmatched_contour_idx = [d4['idx'] for d4 in contours_list if d4['idx'] not in matched_contour_idx]\n",
    "\n",
    "#             if not unmatched_contour_idx:\n",
    "#                 break\n",
    "\n",
    "#             unmatched_contour = np.array([d for d in contours_list if d['idx'] in unmatched_contour_idx])\n",
    "#             recursive_contour_list = find_chars(unmatched_contour)\n",
    "\n",
    "#             for idx in recursive_contour_list:\n",
    "#                 matched_result_idx.append(idx)\n",
    "\n",
    "#             break\n",
    "\n",
    "#         return matched_result_idx\n",
    "\n",
    "\n",
    "\n",
    "#     result_idx = find_chars(possible_contours)\n",
    "\n",
    "#     matched_resultx = []\n",
    "\n",
    "#     for idx_list in result_idx:\n",
    "#         matched_resultx.append(np.take(possible_contours , idx_list))\n",
    "\n",
    "\n",
    "\n",
    "#     temp_resultx = np.zeros((img_cropped.shape[0],img_cropped.shape[1]),dtype=np.uint8)\n",
    "#     for r in matched_resultx:\n",
    "#         for d in r:\n",
    "            \n",
    "#             cv.rectangle(temp_resultx , pt1=(d['x'],d['y']) , pt2=(d['x']+d['w'] , d['y']+d['h']) , color=(255,255,255) , thickness=2)\n",
    "            \n",
    "#     temp_result = np.zeros((img_cropped.shape[0],img_cropped.shape[1]),dtype=np.uint8)\n",
    "\n",
    "#     matched_result = get_largest_contour_list(matched_resultx)\n",
    "\n",
    "#     for r in matched_result:\n",
    "#         for d in r:\n",
    "            \n",
    "#             cv.rectangle(temp_result , pt1=(d['x'],d['y']) , pt2=(d['x']+d['w'] , d['y']+d['h']) , color=(255,255,255) , thickness=cv.FILLED)\n",
    "\n",
    "#     return apply_mask(cropped , temp_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hog_features(img):\n",
    "    target_img_size = (32, 32) # fix image size because classification algorithms THAT WE WILL USE HERE expect that\n",
    "    img = cv2.resize(img, target_img_size)\n",
    "    win_size = (32, 32)\n",
    "    cell_size = (4, 4)\n",
    "    block_size_in_cells = (2, 2)\n",
    "    \n",
    "    block_size = (block_size_in_cells[1] * cell_size[1], block_size_in_cells[0] * cell_size[0])\n",
    "    block_stride = (cell_size[1], cell_size[0])\n",
    "    nbins = 9  # Number of orientation bins\n",
    "    hog = cv2.HOGDescriptor(win_size, block_size, block_stride, cell_size, nbins)\n",
    "    h = hog.compute(img)\n",
    "    h = h.flatten()\n",
    "    return h.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "\n",
    "    path_to_dataset = r'digits_dataset'\n",
    "\n",
    "    features = []\n",
    "    labels = []\n",
    "\n",
    "    # Iterate through subfolders (assume folder name is the label)\n",
    "    for label_folder in os.listdir(path_to_dataset):\n",
    "        label_path = os.path.join(path_to_dataset, label_folder)\n",
    "        \n",
    "        # Ensure it's a directory (skip files)\n",
    "        if not os.path.isdir(label_path):\n",
    "            continue\n",
    "        \n",
    "        # Extract label from the folder name\n",
    "        label = label_folder\n",
    "        \n",
    "        # Iterate through the images in the folder\n",
    "        for img_filename in os.listdir(label_path):\n",
    "            if img_filename.lower().endswith('.jpg'):  # Process only .jpg files\n",
    "                img_path = os.path.join(label_path, img_filename)\n",
    "                \n",
    "                # Read image\n",
    "                img = cv2.imread(img_path)\n",
    "                \n",
    "                # Extract features (e.g., HOG features)\n",
    "                features.append(extract_hog_features(img))\n",
    "                \n",
    "                # Append the label\n",
    "                labels.append(label)\n",
    "                \n",
    "                # Show an update every 1,000 images\n",
    "                if len(features) % 1000 == 0:\n",
    "                    print(f\"[INFO] Processed {len(features)} images so far.\")\n",
    "\n",
    "    return features, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset. This will take time ...\n",
      "[INFO] Processed 1000 images so far.\n",
      "[INFO] Processed 2000 images so far.\n",
      "[INFO] Processed 3000 images so far.\n",
      "[INFO] Processed 4000 images so far.\n",
      "[INFO] Processed 5000 images so far.\n",
      "[INFO] Processed 6000 images so far.\n",
      "[INFO] Processed 7000 images so far.\n",
      "[INFO] Processed 8000 images so far.\n",
      "[INFO] Processed 9000 images so far.\n",
      "[INFO] Processed 10000 images so far.\n",
      "[INFO] Processed 11000 images so far.\n",
      "[INFO] Processed 12000 images so far.\n",
      "[INFO] Processed 13000 images so far.\n",
      "[INFO] Processed 14000 images so far.\n",
      "[INFO] Processed 15000 images so far.\n",
      "[INFO] Processed 16000 images so far.\n",
      "[INFO] Processed 17000 images so far.\n",
      "[INFO] Processed 18000 images so far.\n",
      "[INFO] Processed 19000 images so far.\n",
      "[INFO] Processed 20000 images so far.\n",
      "[INFO] Processed 21000 images so far.\n",
      "[INFO] Processed 22000 images so far.\n",
      "[INFO] Processed 23000 images so far.\n",
      "[INFO] Processed 24000 images so far.\n",
      "[INFO] Processed 25000 images so far.\n",
      "[INFO] Processed 26000 images so far.\n",
      "[INFO] Processed 27000 images so far.\n",
      "[INFO] Processed 28000 images so far.\n",
      "[INFO] Processed 29000 images so far.\n",
      "[INFO] Processed 30000 images so far.\n",
      "[INFO] Processed 31000 images so far.\n",
      "[INFO] Processed 32000 images so far.\n",
      "[INFO] Processed 33000 images so far.\n",
      "[INFO] Processed 34000 images so far.\n",
      "[INFO] Processed 35000 images so far.\n",
      "Finished loading dataset.\n",
      "############## Training KNN ##############\n",
      "KNN accuracy: 99.56338028169014 %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nYou should get the following test accuracies the first time \\n\\nKNN accuracy ~ 99.69014084507042 %\\n\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifiers = {\n",
    "    'KNN': KNeighborsClassifier(n_neighbors=7)\n",
    "}\n",
    "\n",
    "def run_experiment():\n",
    "    # We are going to fix the random seed to make our experiments reproducible \n",
    "    # since some algorithms use pseudorandom generators\n",
    "    random_seed = 42  \n",
    "    random.seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "    \n",
    "    # Load dataset with extracted features\n",
    "    print('Loading dataset. This will take time ...')\n",
    "    features, labels = load_dataset()\n",
    "    print('Finished loading dataset.')\n",
    "    \n",
    "    # Since we don't want to know the performance of our classifier on images it has seen before\n",
    "    # we are going to withhold some images that we will test the classifier on after training \n",
    "    train_features, test_features, train_labels, test_labels = train_test_split(\n",
    "        features, labels, test_size=0.2, random_state=random_seed)\n",
    "    \n",
    "    for model_name, model in classifiers.items():\n",
    "        print('############## Training', model_name, \"##############\")\n",
    "        # Train the model only on the training features\n",
    "        model.fit(train_features, train_labels)\n",
    "        \n",
    "        # Test the model on images it hasn't seen before\n",
    "        accuracy = model.score(test_features, test_labels)\n",
    "        \n",
    "        print(model_name, 'accuracy:', accuracy*100, '%')\n",
    "\n",
    "run_experiment()\n",
    "\"\"\"\n",
    "You should get the following test accuracies the first time \n",
    "\n",
    "KNN accuracy ~ 99.69014084507042 %\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_and_filter_image(image, white_threshold=0.004):\n",
    "\n",
    "    height, width = image.shape\n",
    "    \n",
    "    # split horizontally into 4\n",
    "    part_width = width // 4   \n",
    "    accepted_parts = []\n",
    "    \n",
    "    # Loop through each part \n",
    "    for i in range(4):\n",
    "        \n",
    "        x_start = i * part_width\n",
    "        x_end = (i + 1) * part_width if i != 3 else width  # Ensure the last part includes the remainder\n",
    "        \n",
    "        part = image[:, x_start:x_end]  # Take the entire height for each part\n",
    "        # Count white pixels\n",
    "        num_white = np.sum(part == 255)\n",
    "        \n",
    "        # Calculate the fraction of white pixels in the part\n",
    "        white_fraction = num_white / (part.size)\n",
    "        # If the fraction of white pixels is above the threshold, keep the part\n",
    "        if white_fraction >= white_threshold:\n",
    "            accepted_parts.append(part)\n",
    "    \n",
    "    # Combine accepted parts into one image\n",
    "    if accepted_parts:\n",
    "        combined_image = np.concatenate(accepted_parts, axis=1) \n",
    "    else:\n",
    "        # If no parts meet the threshold, return an empty array\n",
    "        combined_image = np.zeros((height, width), dtype=np.uint8)\n",
    "    \n",
    "    return combined_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_characters(image):\n",
    "    # Apply binary thresholding\n",
    "    _, binary_image = cv2.threshold(image,150, 255, cv2.THRESH_BINARY_INV)\n",
    "    \n",
    "    # Find contours of the characters\n",
    "    contours, _ = cv2.findContours(binary_image, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    # Sort contours from left to right\n",
    "    sorted_contours = sorted(contours, key=lambda ctr: cv2.boundingRect(ctr)[0])\n",
    "\n",
    "    # Initialize list to store segmented characters\n",
    "    segmented_characters = []\n",
    "    \n",
    "    width, height = binary_image.shape\n",
    "\n",
    "    # Define kernel size for dilation and erosion\n",
    "    kernel1 = np.ones((3, 3), np.uint8) \n",
    "\n",
    "\n",
    "    # Loop through contours and extract individual characters\n",
    "    for i, contour in enumerate(sorted_contours):\n",
    "        x, y, w, h = cv2.boundingRect(contour)\n",
    "        \n",
    "        # Filter out small contours\n",
    "        if w > width/8 and h > height/10 and w < width*7/8:\n",
    "            char_image = binary_image[y:y+h, x:x+w]\n",
    "            \n",
    "            # Apply dilation (expands white regions)\n",
    "            dilated_image = cv2.dilate(char_image, kernel1, iterations=1)\n",
    "            \n",
    "            # Apply erosion (shrinks white regions)\n",
    "            eroded_image = cv2.erode(dilated_image, kernel1, iterations=1)\n",
    "            \n",
    "            # Store the result of erosion or dilation or both\n",
    "            segmented_characters.append(char_image)\n",
    "            \n",
    "\n",
    "    return segmented_characters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifyChars(segmented):\n",
    "    knn = classifiers['KNN']\n",
    "    detected =[]\n",
    "    for i, char_image in enumerate(segmented):\n",
    "        features = extract_hog_features(char_image)\n",
    "        detected.append(knn.predict([features]))\n",
    "    \n",
    "    return detected\n",
    "\n",
    "# image = cv2.imread('../trial-2/outputs/output-4.png')[:,:,1]\n",
    "# chars = classifyChars(segment_characters(image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ALLOWED_CHARACTERS = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789\"\n",
    "#     # Define kernel size for dilation and erosion\n",
    "# kernel2 = np.ones((1, 2), np.uint8)  # You can adjust the size (3x3 is a typical choice)\n",
    "\n",
    "# def preprocess_image(image):\n",
    "#     # Resize the image to make characters larger if necessary\n",
    "#     resized = cv2.resize(image, None, fx=2, fy=2, interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "#     # Apply erosion (shrinks white regions)\n",
    "#     eroded_image = cv2.erode(resized, kernel2, iterations=1)\n",
    "#     return eroded_image\n",
    "\n",
    "# def classify_characters_with_ocr(segmented_characters):\n",
    "#     recognized_texts = []\n",
    "\n",
    "#     # Loop through segmented characters and apply OCR\n",
    "#     for i, char_image in enumerate(segmented_characters):\n",
    "#         # Preprocess the character image\n",
    "#         preprocessed_image = preprocess_image(char_image)\n",
    "#         # Use Tesseract OCR to recognize the character\n",
    "#         show_images([preprocessed_image])\n",
    "#         text = pytesseract.image_to_string(preprocessed_image, config='--psm 10 --oem 3 -c tessedit_char_whitelist=' + ALLOWED_CHARACTERS)\n",
    "#         if len(text) > 0:\n",
    "#             # Take only the first valid character from the recognized text\n",
    "#             text = text[0]\n",
    "\n",
    "#         recognized_texts.append(text)\n",
    "\n",
    "#     return recognized_texts\n",
    "\n",
    "# recognized_texts = classify_characters_with_ocr(segmented_characters)\n",
    "\n",
    "# # Print the recognized characters\n",
    "# for i, text in enumerate(recognized_texts):\n",
    "#     print(f\"Character {i}: {text}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "unordered_map = {}\n",
    "\n",
    "# assume parking for 1 second = 15 pounds (hanghayyar el arqam di bas for testing purposes)\n",
    "\n",
    "def calculate_fee(plate_number):\n",
    "    current_time = time.time()  \n",
    "    \n",
    "    # Check if the string is already in the unordered map\n",
    "    if plate_number in unordered_map:\n",
    "        # Calculate the time difference\n",
    "        saved_time = unordered_map.pop(plate_number)\n",
    "        time_diff = current_time - saved_time\n",
    "        print(f\"Car with plate '{plate_number}'. exited after: {time_diff:.2f} seconds., Fee = {time_diff * 15:.2f}\")\n",
    "        return f\"{time_diff * 15:.2f}\"\n",
    "    else:\n",
    "        # Insert the string with the current time\n",
    "        unordered_map[plate_number] = current_time\n",
    "        print(f\"Car with plate '{plate_number}' just entered the parking\")\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app 'PlateDetect'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5001\n",
      "Press CTRL+C to quit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset. This will take time ...\n",
      "[INFO] Processed 1000 images so far.\n",
      "[INFO] Processed 2000 images so far.\n",
      "[INFO] Processed 3000 images so far.\n",
      "[INFO] Processed 4000 images so far.\n",
      "[INFO] Processed 5000 images so far.\n",
      "[INFO] Processed 6000 images so far.\n",
      "[INFO] Processed 7000 images so far.\n",
      "[INFO] Processed 8000 images so far.\n",
      "[INFO] Processed 9000 images so far.\n",
      "[INFO] Processed 10000 images so far.\n",
      "[INFO] Processed 11000 images so far.\n",
      "[INFO] Processed 12000 images so far.\n",
      "[INFO] Processed 13000 images so far.\n",
      "[INFO] Processed 14000 images so far.\n",
      "[INFO] Processed 15000 images so far.\n",
      "[INFO] Processed 16000 images so far.\n",
      "[INFO] Processed 17000 images so far.\n",
      "[INFO] Processed 18000 images so far.\n",
      "[INFO] Processed 19000 images so far.\n",
      "[INFO] Processed 20000 images so far.\n",
      "[INFO] Processed 21000 images so far.\n",
      "[INFO] Processed 22000 images so far.\n",
      "[INFO] Processed 23000 images so far.\n",
      "[INFO] Processed 24000 images so far.\n",
      "[INFO] Processed 25000 images so far.\n",
      "[INFO] Processed 26000 images so far.\n",
      "[INFO] Processed 27000 images so far.\n",
      "[INFO] Processed 28000 images so far.\n",
      "[INFO] Processed 29000 images so far.\n",
      "[INFO] Processed 30000 images so far.\n",
      "[INFO] Processed 31000 images so far.\n",
      "[INFO] Processed 32000 images so far.\n",
      "[INFO] Processed 33000 images so far.\n",
      "[INFO] Processed 34000 images so far.\n",
      "[INFO] Processed 35000 images so far.\n",
      "Finished loading dataset.\n",
      "############## Training KNN ##############\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [22/Dec/2024 21:23:37] \"POST /train_model HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN accuracy: 99.56338028169014 %\n",
      "<FileStorage: 'Screenshot 2024-12-21 at 10.53.25\\u202fPM.png' ('image/png')>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [22/Dec/2024 21:23:44] \"POST /extract_plate HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIRKB1L8\n",
      "Car with plate 'TIRKB1L8' just entered the parking\n",
      "<FileStorage: 'Screenshot 2024-12-21 at 10.53.25\\u202fPM.png' ('image/png')>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [22/Dec/2024 21:23:49] \"POST /extract_plate HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIRKB1L8\n",
      "Car with plate 'TIRKB1L8'. exited after: 5.21 seconds., Fee = 78.21\n",
      "<FileStorage: 'Screenshot 2024-12-21 at 10.53.25\\u202fPM.png' ('image/png')>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [22/Dec/2024 21:23:50] \"POST /extract_plate HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIRKB1L8\n",
      "Car with plate 'TIRKB1L8' just entered the parking\n",
      "<FileStorage: 'Screenshot 2024-12-21 at 10.53.25\\u202fPM.png' ('image/png')>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [22/Dec/2024 21:23:52] \"POST /extract_plate HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIRKB1L8\n",
      "Car with plate 'TIRKB1L8'. exited after: 1.73 seconds., Fee = 25.98\n",
      "<FileStorage: 'Screenshot 2024-12-21 at 10.53.25\\u202fPM.png' ('image/png')>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [22/Dec/2024 21:23:57] \"POST /extract_plate HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIRKB1L8\n",
      "Car with plate 'TIRKB1L8' just entered the parking\n",
      "<FileStorage: 'Screenshot 2024-12-21 at 10.53.25\\u202fPM.png' ('image/png')>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [22/Dec/2024 21:24:00] \"POST /extract_plate HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIRKB1L8\n",
      "Car with plate 'TIRKB1L8'. exited after: 3.84 seconds., Fee = 57.56\n",
      "<FileStorage: 'Screenshot 2024-12-21 at 10.53.25\\u202fPM.png' ('image/png')>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [22/Dec/2024 21:24:06] \"POST /extract_plate HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIRKB1L8\n",
      "Car with plate 'TIRKB1L8' just entered the parking\n",
      "<FileStorage: 'Screenshot 2024-12-21 at 10.53.25\\u202fPM.png' ('image/png')>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [22/Dec/2024 21:24:25] \"POST /extract_plate HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIRKB1L8\n",
      "Car with plate 'TIRKB1L8'. exited after: 18.99 seconds., Fee = 284.83\n",
      "<FileStorage: 'Screenshot 2024-12-21 at 10.53.25\\u202fPM.png' ('image/png')>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [22/Dec/2024 21:24:26] \"POST /extract_plate HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIRKB1L8\n",
      "Car with plate 'TIRKB1L8' just entered the parking\n",
      "<FileStorage: 'Screenshot 2024-12-21 at 10.53.25\\u202fPM.png' ('image/png')>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [22/Dec/2024 21:24:27] \"POST /extract_plate HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIRKB1L8\n",
      "Car with plate 'TIRKB1L8'. exited after: 0.86 seconds., Fee = 12.97\n",
      "<FileStorage: 'Screenshot 2024-12-21 at 10.53.25\\u202fPM.png' ('image/png')>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [22/Dec/2024 21:24:40] \"POST /extract_plate HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIRKB1L8\n",
      "Car with plate 'TIRKB1L8' just entered the parking\n",
      "<FileStorage: 'Screenshot 2024-12-21 at 10.53.25\\u202fPM.png' ('image/png')>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [22/Dec/2024 21:24:40] \"POST /extract_plate HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIRKB1L8\n",
      "Car with plate 'TIRKB1L8'. exited after: 0.83 seconds., Fee = 12.46\n",
      "<FileStorage: 'Screenshot 2024-12-21 at 10.53.25\\u202fPM.png' ('image/png')>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [22/Dec/2024 21:24:41] \"POST /extract_plate HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIRKB1L8\n",
      "Car with plate 'TIRKB1L8' just entered the parking\n",
      "<FileStorage: 'Screenshot 2024-12-21 at 10.53.25\\u202fPM.png' ('image/png')>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [22/Dec/2024 21:24:42] \"POST /extract_plate HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIRKB1L8\n",
      "Car with plate 'TIRKB1L8'. exited after: 0.80 seconds., Fee = 11.96\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "from flask_cors import CORS\n",
    "import cv2\n",
    "import pytesseract\n",
    "import os\n",
    "import threading\n",
    "\n",
    "server_thread = None\n",
    "stop_flag = threading.Event()\n",
    "\n",
    "app = Flask(\"PlateDetect\")\n",
    "CORS(app)\n",
    "\n",
    "@app.route('/extract_plate', methods=['POST'])\n",
    "def extract_plate():\n",
    "    try:\n",
    "        image_file = request.files['image']\n",
    "        image_path = \"uploaded_image.jpg\"\n",
    "        image_file.save(image_path)\n",
    "        print(image_file)\n",
    "        # image = cv2.imread(image_path)\n",
    "        # image = process_image(image)\n",
    "        image = cv2.imread('../trial-2/outputs/output-4.png')[:,:,1] #TODO:replace with actual imag\n",
    "        segmented_chars = segment_characters(image)\n",
    "        classified = classifyChars(segmented_chars)\n",
    "        plateNumber = ''.join([item[0] for item in classified])\n",
    "        print(plateNumber)\n",
    "\n",
    "        # Clean up\n",
    "        os.remove(image_path)\n",
    "\n",
    "        # Calculate fees\n",
    "        fees = calculate_fee(plateNumber)\n",
    "        return jsonify({\"plate\": plateNumber, \"fees\": fees})\n",
    "    except Exception as e:\n",
    "        return jsonify({\"error\": str(e)}), 500\n",
    "\n",
    "@app.route('/train_model', methods=['POST'])\n",
    "def train_model():\n",
    "    run_experiment()\n",
    "    return jsonify({\"message\": \"Model training started.\"})\n",
    "\n",
    "def run_flask():\n",
    "    while not stop_flag.is_set():\n",
    "        app.run(debug=False, use_reloader=False, port=5001)\n",
    "\n",
    "server_thread = threading.Thread(target=run_flask)\n",
    "server_thread.daemon = True\n",
    "server_thread.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app 'PlateDetect'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset. This will take time ...\n",
      "[INFO] Processed 1000 images so far.\n",
      "[INFO] Processed 2000 images so far.\n",
      "[INFO] Processed 3000 images so far.\n",
      "[INFO] Processed 4000 images so far.\n",
      "[INFO] Processed 5000 images so far.\n",
      "[INFO] Processed 6000 images so far.\n",
      "[INFO] Processed 7000 images so far.\n",
      "[INFO] Processed 8000 images so far.\n",
      "[INFO] Processed 9000 images so far.\n",
      "[INFO] Processed 10000 images so far.\n",
      "[INFO] Processed 11000 images so far.\n",
      "[INFO] Processed 12000 images so far.\n",
      "[INFO] Processed 13000 images so far.\n",
      "[INFO] Processed 14000 images so far.\n",
      "[INFO] Processed 15000 images so far.\n",
      "[INFO] Processed 16000 images so far.\n",
      "[INFO] Processed 17000 images so far.\n",
      "[INFO] Processed 18000 images so far.\n",
      "[INFO] Processed 19000 images so far.\n",
      "[INFO] Processed 20000 images so far.\n",
      "[INFO] Processed 21000 images so far.\n",
      "[INFO] Processed 22000 images so far.\n",
      "[INFO] Processed 23000 images so far.\n",
      "[INFO] Processed 24000 images so far.\n",
      "[INFO] Processed 25000 images so far.\n",
      "[INFO] Processed 26000 images so far.\n",
      "[INFO] Processed 27000 images so far.\n",
      "[INFO] Processed 28000 images so far.\n",
      "[INFO] Processed 29000 images so far.\n",
      "[INFO] Processed 30000 images so far.\n",
      "[INFO] Processed 31000 images so far.\n",
      "[INFO] Processed 32000 images so far.\n",
      "[INFO] Processed 33000 images so far.\n",
      "[INFO] Processed 34000 images so far.\n",
      "[INFO] Processed 35000 images so far.\n",
      "Finished loading dataset.\n",
      "############## Training KNN ##############\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [22/Dec/2024 21:20:13] \"POST /train_model HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN accuracy: 99.56338028169014 %\n",
      "<FileStorage: 'Screenshot 2024-12-21 at 10.53.25\\u202fPM.png' ('image/png')>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [22/Dec/2024 21:20:30] \"POST /extract_plate HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIRKB1L8\n",
      "Car with plate 'TIRKB1L8' just entered the parking\n"
     ]
    }
   ],
   "source": [
    "# if server_thread is not None:\n",
    "#     stop_flag.set()  # Signal the thread to stop\n",
    "#     server_thread.join()  # Wait for the thread to finish\n",
    "#     print(\"Flask server stopped.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
